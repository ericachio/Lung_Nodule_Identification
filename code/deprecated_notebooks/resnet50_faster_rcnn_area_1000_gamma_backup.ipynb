{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from skimage import io\n",
    "import time\n",
    "import ast \n",
    "from PIL import *\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>imageName</th>\n",
       "      <th>SOPInstanceUID</th>\n",
       "      <th>boxes</th>\n",
       "      <th>areas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000172.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.358253510231...</td>\n",
       "      <td>[[337.0, 189.0, 376.0, 224.0]]</td>\n",
       "      <td>1365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000174.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.332771657417...</td>\n",
       "      <td>[[335.0, 186.0, 375.0, 225.0]]</td>\n",
       "      <td>1560.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000019.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.133221309761...</td>\n",
       "      <td>[[333.0, 189.0, 369.0, 219.0]]</td>\n",
       "      <td>1080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000175.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.291261298826...</td>\n",
       "      <td>[[334.0, 189.0, 374.0, 218.0]]</td>\n",
       "      <td>1160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000033.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.122854262319...</td>\n",
       "      <td>[[112.0, 139.0, 156.0, 189.0]]</td>\n",
       "      <td>2200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000119.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.787525847146...</td>\n",
       "      <td>[[316.0, 313.0, 350.0, 349.0]]</td>\n",
       "      <td>1224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000081.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.216953803063...</td>\n",
       "      <td>[[319.0, 307.0, 350.0, 344.0]]</td>\n",
       "      <td>1147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000056.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.182307837643...</td>\n",
       "      <td>[[313.0, 308.0, 352.0, 346.0]]</td>\n",
       "      <td>1482.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000056.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.182307837643...</td>\n",
       "      <td>[[181.0, 173.0, 208.0, 211.0]]</td>\n",
       "      <td>1026.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000021.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.225230651036...</td>\n",
       "      <td>[[314.0, 308.0, 352.0, 345.0]]</td>\n",
       "      <td>1406.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>476 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  path   imageName  \\\n",
       "0    /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000172.png   \n",
       "1    /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000174.png   \n",
       "2    /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000019.png   \n",
       "3    /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000175.png   \n",
       "4    /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000033.png   \n",
       "..                                                 ...         ...   \n",
       "471  /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000119.png   \n",
       "472  /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000081.png   \n",
       "473  /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000056.png   \n",
       "474  /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000056.png   \n",
       "475  /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000021.png   \n",
       "\n",
       "                                        SOPInstanceUID  \\\n",
       "0    1.3.6.1.4.1.14519.5.2.1.6279.6001.358253510231...   \n",
       "1    1.3.6.1.4.1.14519.5.2.1.6279.6001.332771657417...   \n",
       "2    1.3.6.1.4.1.14519.5.2.1.6279.6001.133221309761...   \n",
       "3    1.3.6.1.4.1.14519.5.2.1.6279.6001.291261298826...   \n",
       "4    1.3.6.1.4.1.14519.5.2.1.6279.6001.122854262319...   \n",
       "..                                                 ...   \n",
       "471  1.3.6.1.4.1.14519.5.2.1.6279.6001.787525847146...   \n",
       "472  1.3.6.1.4.1.14519.5.2.1.6279.6001.216953803063...   \n",
       "473  1.3.6.1.4.1.14519.5.2.1.6279.6001.182307837643...   \n",
       "474  1.3.6.1.4.1.14519.5.2.1.6279.6001.182307837643...   \n",
       "475  1.3.6.1.4.1.14519.5.2.1.6279.6001.225230651036...   \n",
       "\n",
       "                              boxes   areas  \n",
       "0    [[337.0, 189.0, 376.0, 224.0]]  1365.0  \n",
       "1    [[335.0, 186.0, 375.0, 225.0]]  1560.0  \n",
       "2    [[333.0, 189.0, 369.0, 219.0]]  1080.0  \n",
       "3    [[334.0, 189.0, 374.0, 218.0]]  1160.0  \n",
       "4    [[112.0, 139.0, 156.0, 189.0]]  2200.0  \n",
       "..                              ...     ...  \n",
       "471  [[316.0, 313.0, 350.0, 349.0]]  1224.0  \n",
       "472  [[319.0, 307.0, 350.0, 344.0]]  1147.0  \n",
       "473  [[313.0, 308.0, 352.0, 346.0]]  1482.0  \n",
       "474  [[181.0, 173.0, 208.0, 211.0]]  1026.0  \n",
       "475  [[314.0, 308.0, 352.0, 345.0]]  1406.0  \n",
       "\n",
       "[476 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset / old csv\n",
    "# df = pd.read_csv('temp.csv')\n",
    "\n",
    "# area larger than 1000\n",
    "df = pd.read_csv('area_larger_than_1000.csv')\n",
    "\n",
    "# full dataset \n",
    "# df = pd.read_csv('large_nodules_png_bounding_box.csv')\n",
    "\n",
    "# subset of dataset (25%)\n",
    "# _ , X_random, = train_test_split(df, test_size=0.25, random_state=0)\n",
    "# X_random.to_csv('subset.csv',index=False)\n",
    "# df = X_random\n",
    "\n",
    "# df.iloc[0,0]\n",
    "\n",
    "# delete smaller boxes\n",
    "# df = pd.read_csv('large_nodules_png_bounding_box.csv')\n",
    "# df_filtered = df[df['areas'] >= 50]  \n",
    "# df_filtered.to_csv(\"df_filtered.csv\", index = False)\n",
    "# df = pd.read_csv(\"df_filtered.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2880.0 1008.0\n"
     ]
    }
   ],
   "source": [
    "print(max(df['areas']), min(df['areas']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onlyMax = []\n",
    "# for index, row in df.iterrows():\n",
    "#     img_name = (row.path)\n",
    "#     image = cv2.imread(img_name) \n",
    "# #     print('og: Min: %.3f, Max: %.3f' % (image.min(), image.max()))\n",
    "#     if (image.max() < 255.00):\n",
    "#         onlyMax.append(row)\n",
    "# df = pd.DataFrame(onlyMax)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n"
     ]
    }
   ],
   "source": [
    "def adjust_gamma(image, gamma=1.0):\n",
    "    # build a lookup table mapping the pixel values [0, 255] to\n",
    "    # their adjusted gamma values\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255\n",
    "        for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    # apply gamma correction using the lookup table\n",
    "    return cv2.LUT(image, table)\n",
    "count = 0\n",
    "mean = []\n",
    "normalized_mean = []\n",
    "gamma_mean = []\n",
    "\n",
    "normalized_gamma = []\n",
    "for index, row in df.iterrows():\n",
    "    img_name = (row.path)\n",
    "    image = cv2.imread(img_name) \n",
    "    \n",
    "    if (np.mean(image) < 50):\n",
    "        test = adjust_gamma(image, 12)\n",
    "        boxes = ast.literal_eval(row.boxes)\n",
    "        for i in boxes:\n",
    "            ymin, ymax = i[1],i[3] #ymin ymax        \n",
    "            xmin, xmax = (i[0]), (i[2]) #xmin, xmax\n",
    "        normalized = (test - test.min())/(test.max() - test.min()) * 255.0\n",
    "#         fig, axarr = plt.subplots(1,3, figsize=(10,10))\n",
    "#         print(np.mean(image), np.mean(test), np.mean(normalized))\n",
    "#         rect = patches.Rectangle((xmin, ymin ),(xmax - xmin),(ymax - ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "#         axarr[0].add_patch(rect)\n",
    "#         axarr[0].imshow(image, cmap='gray')\n",
    "#         axarr[1].imshow(test, cmap='gray')\n",
    "#         axarr[2].imshow(normalized, cmap='gray')\n",
    "#         plt.show()\n",
    "        if (np.mean(test) > 50):\n",
    "            count += 1\n",
    "#             print(test.max(), test.min())\n",
    "            gamma_mean.append(row)\n",
    "    else:\n",
    "        test = adjust_gamma(image, 0.5)\n",
    "        boxes = ast.literal_eval(row.boxes)\n",
    "        for i in boxes:\n",
    "            ymin, ymax = i[1],i[3] #ymin ymax        \n",
    "            xmin, xmax = (i[0]), (i[2]) #xmin, xmax\n",
    "#         fig, axarr = plt.subplots(1,2, figsize=(10,10))\n",
    "#         print(row.path)\n",
    "#         print(np.mean(image), np.mean(test))\n",
    "#         rect = patches.Rectangle((xmin, ymin ),(xmax - xmin),(ymax - ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "#         axarr[0].add_patch(rect)\n",
    "#         axarr[0].imshow(image, cmap='gray')\n",
    "#         axarr[1].imshow(test, cmap='gray')\n",
    "#         plt.show()\n",
    "        if (np.mean(test) < 125):\n",
    "            count += 1\n",
    "#             print(test.max(), test.min())\n",
    "            gamma_mean.append(row)\n",
    "print(count)\n",
    "#LIDC-IDRI-0702\n",
    "#LIDC-IDRI-0976\n",
    "#LIDC-IDRI-0340"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>imageName</th>\n",
       "      <th>SOPInstanceUID</th>\n",
       "      <th>boxes</th>\n",
       "      <th>areas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000133.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.110311584742...</td>\n",
       "      <td>[[187.0, 208.0, 213.0, 248.0]]</td>\n",
       "      <td>1040.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000101.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.189665398752...</td>\n",
       "      <td>[[188.0, 208.0, 214.0, 247.0]]</td>\n",
       "      <td>1014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000041.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.297813206491...</td>\n",
       "      <td>[[299.0, 346.0, 337.0, 389.0]]</td>\n",
       "      <td>1634.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000003.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.261151233960...</td>\n",
       "      <td>[[300.0, 349.0, 335.0, 385.0]]</td>\n",
       "      <td>1260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000080.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.299410838455...</td>\n",
       "      <td>[[297.0, 346.0, 334.0, 387.0]]</td>\n",
       "      <td>1517.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000177.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.336351461606...</td>\n",
       "      <td>[[169.0, 328.0, 202.0, 379.0]]</td>\n",
       "      <td>1683.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000258.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.293342189867...</td>\n",
       "      <td>[[169.0, 327.0, 203.0, 379.0]]</td>\n",
       "      <td>1768.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000275.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.186316898017...</td>\n",
       "      <td>[[178.0, 331.0, 204.0, 376.0]]</td>\n",
       "      <td>1170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000133.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.275507735707...</td>\n",
       "      <td>[[169.0, 327.0, 204.0, 379.0]]</td>\n",
       "      <td>1820.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...</td>\n",
       "      <td>000055.png</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.240297906592...</td>\n",
       "      <td>[[175.0, 332.0, 206.0, 381.0]]</td>\n",
       "      <td>1519.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  path   imageName  \\\n",
       "14   /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000133.png   \n",
       "15   /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000101.png   \n",
       "16   /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000041.png   \n",
       "17   /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000003.png   \n",
       "18   /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000080.png   \n",
       "..                                                 ...         ...   \n",
       "462  /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000177.png   \n",
       "463  /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000258.png   \n",
       "464  /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000275.png   \n",
       "465  /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000133.png   \n",
       "466  /scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-I...  000055.png   \n",
       "\n",
       "                                        SOPInstanceUID  \\\n",
       "14   1.3.6.1.4.1.14519.5.2.1.6279.6001.110311584742...   \n",
       "15   1.3.6.1.4.1.14519.5.2.1.6279.6001.189665398752...   \n",
       "16   1.3.6.1.4.1.14519.5.2.1.6279.6001.297813206491...   \n",
       "17   1.3.6.1.4.1.14519.5.2.1.6279.6001.261151233960...   \n",
       "18   1.3.6.1.4.1.14519.5.2.1.6279.6001.299410838455...   \n",
       "..                                                 ...   \n",
       "462  1.3.6.1.4.1.14519.5.2.1.6279.6001.336351461606...   \n",
       "463  1.3.6.1.4.1.14519.5.2.1.6279.6001.293342189867...   \n",
       "464  1.3.6.1.4.1.14519.5.2.1.6279.6001.186316898017...   \n",
       "465  1.3.6.1.4.1.14519.5.2.1.6279.6001.275507735707...   \n",
       "466  1.3.6.1.4.1.14519.5.2.1.6279.6001.240297906592...   \n",
       "\n",
       "                              boxes   areas  \n",
       "14   [[187.0, 208.0, 213.0, 248.0]]  1040.0  \n",
       "15   [[188.0, 208.0, 214.0, 247.0]]  1014.0  \n",
       "16   [[299.0, 346.0, 337.0, 389.0]]  1634.0  \n",
       "17   [[300.0, 349.0, 335.0, 385.0]]  1260.0  \n",
       "18   [[297.0, 346.0, 334.0, 387.0]]  1517.0  \n",
       "..                              ...     ...  \n",
       "462  [[169.0, 328.0, 202.0, 379.0]]  1683.0  \n",
       "463  [[169.0, 327.0, 203.0, 379.0]]  1768.0  \n",
       "464  [[178.0, 331.0, 204.0, 376.0]]  1170.0  \n",
       "465  [[169.0, 327.0, 204.0, 379.0]]  1820.0  \n",
       "466  [[175.0, 332.0, 206.0, 381.0]]  1519.0  \n",
       "\n",
       "[320 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(gamma_mean)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def adjust_gamma(image, gamma=1.0):\n",
    "#     # build a lookup table mapping the pixel values [0, 255] to\n",
    "#     # their adjusted gamma values\n",
    "#     invGamma = 1.0 / gamma\n",
    "#     table = np.array([((i / 255.0) ** invGamma) * 255\n",
    "#         for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "#     # apply gamma correction using the lookup table\n",
    "#     return cv2.LUT(image, table)\n",
    "    \n",
    "# def getClosest(target, array):\n",
    "#     closest = -1;\n",
    "#     for element in array:\n",
    "#         if (closest == -1):\n",
    "#             closest = element\n",
    "#         if (abs(target - closest) > abs(element - target)):\n",
    "#             closest = element\n",
    "#     return array.index(closest)\n",
    "\n",
    "# # mean 77.38484446741954 normalized 90.347644293816\n",
    "# for index, row in df.iterrows():\n",
    "#     img_name = (row.path)\n",
    "#     image = cv2.imread(img_name) \n",
    "    \n",
    "#     if (np.mean(image) > 100):\n",
    "#         gammma = [0.25, 0.5]\n",
    "#     elif(np.mean(image) < 50):\n",
    "#         gamma = [10.0, 11.0, 12.0]\n",
    "#     else:\n",
    "#         gamma = [0.5, 1.0, 1.5]\n",
    "#     gamImages = []\n",
    "#     gamMean = []\n",
    "#     for gam in gamma:\n",
    "#         gam_image = adjust_gamma(image, gam)\n",
    "#         gamImages.append(gam_image)\n",
    "#         gamMean.append(np.mean(gam_image))\n",
    "#     indexImage = getClosest(95, gamMean)\n",
    "    \n",
    "#     boxes = ast.literal_eval(row.boxes)\n",
    "#     for i in boxes:\n",
    "#         ymin, ymax = i[1],i[3] #ymin ymax        \n",
    "#         xmin, xmax = (i[0]), (i[2]) #xmin, xmax\n",
    "#     print(\"original mean: \", np.mean(image), \" gamma: \", np.mean(gamMean[indexImage]))\n",
    "#     fig, axarr = plt.subplots(1,2, figsize=(10,10))\n",
    "#     rect = patches.Rectangle((xmin, ymin ),(xmax - xmin),(ymax - ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "#     axarr[0].add_patch(rect)\n",
    "#     axarr[0].imshow(image, cmap='gray')\n",
    "#     axarr[1].imshow(gamImages[indexImage], cmap='gray')\n",
    "\n",
    "# # print(np.mean(mean), np.mean(normalized_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_gamma(image, gamma=1.0):\n",
    "    # build a lookup table mapping the pixel values [0, 255] to\n",
    "    # their adjusted gamma values\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255\n",
    "        for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "    # apply gamma correction using the lookup table\n",
    "    return cv2.LUT(image, table)\n",
    "mean = []\n",
    "normalized_mean = []\n",
    "gamma_mean = []\n",
    "for index, row in df.iterrows():\n",
    "    img_name = (row.path)\n",
    "    image = cv2.imread(img_name) \n",
    "    mean.append(np.mean(image))\n",
    "    print('og: Min: %.3f, Max: %.3f' % (image.min(), image.max()), np.mean(image))\n",
    "#     if (image.max() == 255.00):\n",
    "    if(np.mean(image) > 100):\n",
    "#         test = adjust_gamma(image, 1.0)\n",
    "#         onlyMax.append(row)\n",
    "        test = adjust_gamma(image, .5)\n",
    "    elif (np.mean(image) < 10):\n",
    "        test = adjust_gamma(image, 5)\n",
    "    else:\n",
    "        test = image\n",
    "    gamma_mean.append(test)\n",
    "    boxes = ast.literal_eval(row.boxes)\n",
    "    \n",
    "    for i in boxes:\n",
    "        ymin, ymax = i[1],i[3] #ymin ymax        \n",
    "        xmin, xmax = (i[0]), (i[2]) #xmin, xmax\n",
    "    normalized = (image - image.min())/(image.max() - image.min()) * 255.0\n",
    "    normalized_mean.append(np.mean(normalized))\n",
    "    fig, axarr = plt.subplots(1,3, figsize=(10,10))\n",
    "    rect = patches.Rectangle((xmin, ymin ),(xmax - xmin),(ymax - ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "    axarr[0].add_patch(rect)\n",
    "    axarr[0].imshow(image, cmap='gray')\n",
    "    axarr[1].imshow(test, cmap='gray')\n",
    "    axarr[2].imshow(normalized, cmap='gray')\n",
    "    print('gamma : Min: %.3f, Max: %.3f' % (test.min(), test.max()))\n",
    "    print('normalized : Min: %.3f, Max: %.3f' % (normalized.min(), normalized.max()))\n",
    "    print(\"gamma :\", np.mean(test), \" normalized: \", np.mean(normalized))\n",
    "#     axarr[1].add_patch(rect)\n",
    "    plt.show()\n",
    "print(np.mean(mean), np.mean(normalized_mean))\n",
    "# image =  cv2.imread(\"/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-IDRI-0001/01-01-2000-30178/3000566-03192/000030.png\")\n",
    "# test = adjust_gamma(image, 1.5)\n",
    "\n",
    "# # image = cv2.imread(path) # Read image with cv2\n",
    "# fig, axarr = plt.subplots(1,2, figsize=(10,10))\n",
    "# axarr[0].imshow(image, cmap='gray')\n",
    "# axarr[1].imshow(test, cmap='gray')\n",
    "# plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into 60, 20, 20 \n",
    "train_data, test_data = train_test_split(df, test_size=0.40, random_state=2020)\n",
    "valid_data, test_data = train_test_split(test_data, test_size=0.50, random_state=2020)\n",
    "train_data.index = np.arange(len(train_data))\n",
    "valid_data.index = np.arange(len(valid_data))\n",
    "test_data.index = np.arange(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"large_nodules_train.csv\", index=False)\n",
    "valid_data.to_csv(\"large_nodules_valid.csv\", index=False)\n",
    "test_data.to_csv(\"large_nodules_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomHorizontalFlip(object):\n",
    "\n",
    "    \"\"\"Randomly horizontally flips the Image with the probability *p*\n",
    "    Parameters\n",
    "    ----------\n",
    "    p: float\n",
    "        The probability with which the image is flipped\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndaaray\n",
    "        Flipped image in the numpy format of shape `HxWxC`\n",
    "    numpy.ndarray\n",
    "        Tranformed bounding box co-ordinates of the format `n x 4` where n is\n",
    "        number of bounding boxes and 4 represents `x1,y1,x2,y2` of the box\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "            img_center = np.array(img.shape[:2])[::-1]/2\n",
    "            img_center = np.hstack((img_center, img_center))\n",
    "            if random.random() < self.p:\n",
    "                img = img[:, ::-1, :]\n",
    "                bboxes[:, [0, 2]] += 2*(img_center[[0, 2]] - bboxes[:, [0, 2]])\n",
    "\n",
    "                box_w = abs(bboxes[:, 0] - bboxes[:, 2])\n",
    "\n",
    "                bboxes[:, 0] -= box_w\n",
    "                bboxes[:, 2] += box_w\n",
    "\n",
    "            return img, bboxes\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        landmarks = landmarks * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'landmarks': landmarks}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        landmarks = landmarks - [left, top]\n",
    "\n",
    "        return {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'landmarks': torch.from_numpy(landmarks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "# torchvision models are trained on input images normalized to [0 1] range .ToPILImage() function achives this\n",
    "# additional normalization is required see: http://pytorch.org/docs/master/torchvision/models.html\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "#         transforms.RandomResizedCrop(512),\n",
    "        transforms.CenterCrop(512),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "#         transforms.Normalize([0.18082525, 0.18082525, 0.18082525],[0.1896516, 0.1896516, 0.1896516])])\n",
    "#         transforms.Normalize([0.34390035, 0.34390035, 0.34390035], [0.31606647, 0.31606647, 0.31606647])])\n",
    "\n",
    "\n",
    "validation_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.CenterCrop(512),\n",
    "        transforms.ToTensor()])\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "class Faster_RCNN_Dataloader(Dataset):\n",
    "    \"\"\"Chest X-ray dataset from https://nihcc.app.box.com/v/ChestXray-NIHCC.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file filename information.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # get image path\n",
    "        img_name = (self.data_frame.iloc[idx, 0])\n",
    "        \n",
    "#         # open image \n",
    "#         image = io.imread(img_name)\n",
    "#         # 3 channels\n",
    "#         image = np.array(image)\n",
    "        image = cv2.imread(img_name) # Read image with cv2\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        \n",
    "        # transform image? \n",
    "#         plt.imshow(image)\n",
    "        d = {}\n",
    "        \n",
    "        if (np.mean(image) > 100):\n",
    "            gammma = [0.5]\n",
    "        elif(np.mean(image) < 50):\n",
    "            gamma = [10.0, 11.0, 12.0]\n",
    "        else:\n",
    "            gamma = [0.5, 1.0, 1.5]\n",
    "        gamImages = []\n",
    "        gamMean = []\n",
    "        for gam in gamma:\n",
    "            gam_image = adjust_gamma(image, gam)\n",
    "            gamImages.append(gam_image)\n",
    "            gamMean.append(np.mean(gam_image))\n",
    "        indexImage = getClosest(95, gamMean)\n",
    "        image = gamImages[indexImage]\n",
    "        image = (image - image.min())/(image.max() - image.min()) * 255.0\n",
    "#         print('normalized: Min: %.3f, Max: %.3f' % (image.min(), image.max()))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(np.uint8(image))\n",
    "        \n",
    "            # confirm pixel range is 0-255\n",
    "\n",
    "#             print('tensor: Min: %.3f, Max: %.3f' % (image.min(), image.max()))\n",
    "#             print(image.shape)\n",
    "            d['boxes'] = torch.FloatTensor(ast.literal_eval(self.data_frame.iloc[idx, 3]))\n",
    "            d['labels'] = torch.ones([1], dtype=torch.int64)\n",
    "#             d['labels'] = torch.ones((1, 1),  dtype=torch.int64)\n",
    "#         plt.imshow(image.squeeze().numpy())\n",
    "        else:\n",
    "            d['boxes'] = torch.FloatTensor(ast.literal_eval(self.data_frame.iloc[idx, 3]))\n",
    "            d['labels'] = torch.ones([1], dtype=torch.int64)\n",
    "\n",
    "        \n",
    "        return image, d\n",
    "\n",
    "# change dataloader output to lists.\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    return list(xx), list(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_path = 'large_nodules_train.csv'\n",
    "validate_df_path = 'large_nodules_valid.csv'\n",
    "test_df_path = 'large_nodules_test.csv'\n",
    "\n",
    "transformed_dataset = {'train': Faster_RCNN_Dataloader(train_df_path, transform=train_transform),\n",
    "                       'validate':Faster_RCNN_Dataloader(validate_df_path, transform=validation_transform),\n",
    "                       'test':Faster_RCNN_Dataloader(test_df_path, transform=validation_transform)}\n",
    "bs = 8\n",
    "\n",
    "dataloader = {x: DataLoader(transformed_dataset[x], batch_size=bs,\n",
    "                        shuffle=True, collate_fn = pad_collate, num_workers=0) for x in ['train', 'validate','test']}\n",
    "data_sizes ={x: len(transformed_dataset[x]) for x in ['train', 'validate','test']}\n",
    "\n",
    "\n",
    "# one dataloader / phase, may be too memory intensive...\n",
    "\n",
    "# train_dataset = {'train': Faster_RCNN_Dataloader(train_df_path, transform=train_transform)}\n",
    "# validate_dataset = {'validate': Faster_RCNN_Dataloader(validate_df_path, transform=validation_transform)}\n",
    "# test_dataset = {'test': Faster_RCNN_Dataloader(test_df_path)}\n",
    "\n",
    "# bs = 16 #if bigger, running out of memory in p100\n",
    "\n",
    "# train_dataloader = {x : DataLoader(train_dataset[x], batch_size=bs,\n",
    "#                         shuffle=True,collate_fn = pad_collate, num_workers=0) for x in ['train']}\n",
    "# validate_dataloader = {x : DataLoader(validate_dataset[x], batch_size=bs,\n",
    "#                         shuffle=True,collate_fn = pad_collate, num_workers=0) for x in ['validate']}\n",
    "# test_dataloader = {x : DataLoader(test_dataset[x], batch_size=bs,\n",
    "#                         shuffle=True,collate_fn = pad_collate, num_workers=0) for x in ['test']}\n",
    "# data_sizes ={len(dataset[x]) for x in ['train']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-IDRI-0001/01-01-2000-30178/3000566-03192/000030.png\")\n",
    "# plt.imshow(image, cmap=\"bone\")\n",
    "print(image.format)\n",
    "print(image.size)\n",
    "print(image.mode)\n",
    "\n",
    "path = '/scratch/ebc308/tcia/data_png/LIDC-IDRI/LIDC-IDRI-0001/01-01-2000-30178/3000566-03192/000030.png'\n",
    "boxes = \"[[302.0, 351.0, 329.0, 384.0]]\"\n",
    "\n",
    "def box_png(path, boxes):\n",
    "    boxes = ast.literal_eval(boxes)\n",
    "    for i in boxes:\n",
    "            ymin, ymax = i[1],i[3] #ymin ymax        \n",
    "            xmin, xmax = (i[0]), (i[2]) #xmin, xmax\n",
    "#     im = np.array(Image.open(path), dtype=np.uint8)\n",
    "    image = cv2.imread(path) # Read image with cv2\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    fig,ax = plt.subplots(1, figsize=(15,15))\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    rect = patches.Rectangle((xmin, ymin ),(xmax - xmin),(ymax - ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    plt.show()\n",
    "    \n",
    "box_png(path, boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if dataloader loaded correctly\n",
    "# images1, targets1 = next(iter(train_dataloader['train']))\n",
    "# images2, targets2 = next(iter(validate_dataloader['validate']))\n",
    "# images3, targets3 = next(iter(test_dataloader['test']))\n",
    "\n",
    "# to check dim of list dataloader\n",
    "# len(images) = batch\n",
    "# len(images[0]) = channel\n",
    "# len(images[0]) = height\n",
    "# len(images[0]) = width\n",
    "\n",
    "# testing dataloader \n",
    "images4, targets4 = next(iter(dataloader['train']))\n",
    "\n",
    "# for i in targets4[0]['boxes'].numpy():\n",
    "#     ymin, ymax = i[1],i[3] #ymin ymax        \n",
    "#     xmin, xmax = i[0], i[2] #xmin, xmax\n",
    "# fig,ax = plt.subplots(1, figsize=(5,5))\n",
    "# ax.imshow(images4[0][1].squeeze().numpy(), cmap='gray')\n",
    "# rect = patches.Rectangle((xmin, ymin ),(xmax - xmin),(ymax - ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "# ax.add_patch(rect)\n",
    "# plt.show()\n",
    "\n",
    "for images, targets in dataloader['train']:\n",
    "    for i, img in enumerate(images):\n",
    "        for i in targets[i]['boxes'].numpy():\n",
    "            ymin, ymax = i[1],i[3] #ymin ymax        \n",
    "            xmin, xmax = i[0], i[2] #xmin, xmax\n",
    "            fig,ax = plt.subplots(1, figsize=(5,5))\n",
    "            ax.imshow(img[1].squeeze().numpy(), cmap=\"gray\")\n",
    "            rect = patches.Rectangle((xmin, ymin ),(xmax - xmin),(ymax - ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_mean1 = []\n",
    "pop_std1 = []\n",
    "\n",
    "pop_mean2 = []\n",
    "pop_std2 = []\n",
    "\n",
    "pop_mean3 = []\n",
    "pop_std3 = []\n",
    "for i, data in enumerate(dataloader['train']):\n",
    "    for i in data[0]:\n",
    "\n",
    "        numpy_image = i.numpy()\n",
    "#         print(len(numpy_image))\n",
    "        mean1 = np.mean(numpy_image[0])\n",
    "        mean2 = np.mean(numpy_image[1])\n",
    "        mean3 = np.mean(numpy_image[2])\n",
    "        \n",
    "        std1 = np.std(numpy_image[0])\n",
    "        std2 = np.std(numpy_image[1])\n",
    "        std3 = np.std(numpy_image[2])\n",
    "        \n",
    "        pop_mean1.append(mean1)\n",
    "        pop_mean2.append(mean2)\n",
    "        pop_mean3.append(mean3)\n",
    "        \n",
    "        pop_std1.append(std1)\n",
    "        pop_std2.append(std2)\n",
    "        pop_std3.append(std3)\n",
    "\n",
    "pop_mean1 = np.array(pop_mean1).mean(axis=0)\n",
    "pop_std1 = np.array(pop_std1).mean(axis=0)\n",
    "pop_mean2 = np.array(pop_mean2).mean(axis=0)\n",
    "pop_std2 = np.array(pop_std2).mean(axis=0)\n",
    "pop_mean3 = np.array(pop_mean3).mean(axis=0)\n",
    "pop_std3 = np.array(pop_std3).mean(axis=0)\n",
    "print(pop_mean1,pop_mean2,pop_mean3, pop_std1, pop_std2, pop_std3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (nodule) + background\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "\n",
    "# anchor_generator = AnchorGenerator(sizes=((4, 8, 16, 32, 36, 40, 48, 49, 56, 64, 72),),\n",
    "#                                    aspect_ratios=((0.25, 0.5, 0.75, 1.0, 1.25, 1.5),))\n",
    "\n",
    "# areas > 1000\n",
    "anchor_generator = AnchorGenerator(sizes=((36, 40, 48, 49, 56, 64, 72),),\n",
    "                                   aspect_ratios=((.75, 1.0, 1.25, 1.5),))\n",
    "\n",
    "# anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "#                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[\"0\"],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler,\n",
    "                   rpn_nms_thresh = .9).to(device)\n",
    "\n",
    "# tried to run not pretrained.\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(num_classes=2).to(device)\n",
    "\n",
    "# for multiple nodes\n",
    "torch.nn.DataParallel(model)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4,\n",
    "                                momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.00001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "lambda_func = lambda epoch: 0.5 ** epoch\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_func)\n",
    "\n",
    "# unused \n",
    "cel = nn.MSELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move dict/lists to gpu \n",
    "def move_to(obj, device):\n",
    "    if torch.is_tensor(obj):\n",
    "        return obj.to(device)\n",
    "    elif isinstance(obj, dict):\n",
    "        res = {}\n",
    "        for k, v in obj.items():\n",
    "            res[k] = move_to(v, device)\n",
    "        return res\n",
    "    elif isinstance(obj, list):\n",
    "        res = []\n",
    "        for v in obj:\n",
    "            res.append(move_to(v, device))\n",
    "        return res\n",
    "    else:\n",
    "        raise TypeError(\"Invalid type for move_to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['__background__', 'nodule']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(float(boxA[0]), float(boxB[0]))\n",
    "    yA = max(float(boxA[1]), float(boxB[1]))\n",
    "    xB = min(float(boxA[2]), float(boxB[2]))\n",
    "    yB = min(float(boxA[3]), float(boxB[3]))\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader,optimizer, scheduler, loss_fn, num_epochs = 10, verbose = True):\n",
    "    acc_dict = {'train':[]}\n",
    "    loss_dict = {'train':[], 'validate':[]}\n",
    "    best_acc = 0\n",
    "    phases = ['train','validate']\n",
    "    since = time.time()\n",
    "    \n",
    "    epochLossTrain = list()\n",
    "#     epochAccTrain = list()\n",
    "#     epochLossValidate = list()\n",
    "    epochAccValidate = list()\n",
    "    epochAccBestScore = list()\n",
    "    for i in range(num_epochs):\n",
    "        print('Epoch: {}/{}'.format(i, num_epochs-1))\n",
    "        print('-'*10)\n",
    "        losslist = []\n",
    "        for p in phases:\n",
    "            running_correct = 0\n",
    "            running_loss = 0\n",
    "            running_total = 0\n",
    "            maxIOUs = [-1]\n",
    "            bestScore = [-1]\n",
    "            if p == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()   \n",
    "            num = 0 # calculate which batch --> enumerate not working\n",
    "            loss = 0.0\n",
    "            noPredictions = 0\n",
    "            bestScore = []\n",
    "            list_max = []\n",
    "            for image, target in dataloader[p]:\n",
    "#                 optimizer.zero_grad()\n",
    "                # move data to gpu\n",
    "#                 plt.imshow(image[0].squeeze().numpy())\n",
    "                image = move_to(image,device)\n",
    "                target = move_to(target,device)\n",
    "                if (num % (len(dataloader[p])/4) == 0):\n",
    "                    print('{} set | epoch: {:3d} | {:6d}/{:6d} batches'.format(p, i, num, len(dataloader[p])))\n",
    "                num = num + 1\n",
    "                \n",
    "                if p == 'train':\n",
    "                    # loss = sum of all 4 losses returned\n",
    "                    output = model((image), target)\n",
    "                    loss = sum(loss for loss in output.values())\n",
    "                if p == 'validate':\n",
    "                    # helps with memory \n",
    "                    with torch.no_grad():\n",
    "                        model.eval()  \n",
    "                        output = model((image))\n",
    "                        pred_score = [0] # confidence score\n",
    "                        pred_boxes = [] # bounding box\n",
    "                        pred_class = [] # predicted class\n",
    "                        maxIOUs = [0]\n",
    "                        # loop through output (len(output) = batch size )\n",
    "                        for j, out in enumerate(output):\n",
    "                            # documentation: ['boxes'] = [x1, y1, x2, y2]\n",
    "                            pred_boxes = [[i[0], i[1], i[2], i[3]] for i in list(out['boxes'].cpu().detach().numpy())]\n",
    "                            pred_score = list(out['scores'].cpu().detach().numpy())\n",
    "                            pred_class = [classes[i] for i in list(out['labels'].cpu().numpy())]\n",
    "                            \n",
    "                            # get correct label\n",
    "                            # documentation: # 0 xmin, 1 ymin, 2 xmax, 3 ymax\n",
    "                            t = target[j]['boxes'].cpu().detach().numpy()[0]\n",
    "                            # want to input: x min , y max, x max, y min\n",
    "#                             target_box =  [t[0],t[3],t[2],t[1]]\n",
    "                            target_box =  [t[0],t[1],t[2],t[3]]\n",
    "    \n",
    "                            # right now, get iou of every predicted box\n",
    "                            # and keep max iou \n",
    "                            iou = []\n",
    "                            t = []\n",
    "                            for counting, ind in enumerate(pred_boxes):\n",
    "                                if (bb_intersection_over_union(target_box, ind) > 0):\n",
    "                                    # only keep if iou > 0\n",
    "                                    iou.append(bb_intersection_over_union(target_box, ind))\n",
    "                                t.append(bb_intersection_over_union(target_box, ind))\n",
    "                            # get max of iou of the image, append to max iou\n",
    "                            if (len(iou) > 0):\n",
    "                                maxIOUs.append(max(iou))\n",
    "                            list_max.append(max(t))\n",
    "                                \n",
    "                            max_score_index = pred_score.index(max(pred_score))\n",
    "                            bestScore.append(bb_intersection_over_union(target_box, pred_boxes[max_score_index]))\n",
    "#                             print(j, bb_intersection_over_union(target_box, pred_boxes[max_score_index]))\n",
    "                            # get index of max score - in theory want this one.\n",
    "#                             if (len(pred_score) > 0):\n",
    "#                                 index = pred_score.index(max(pred_score))\n",
    "#                                 bestScore.append(bb_intersection_over_union(target_box, pred_boxes[index]))\n",
    "#                             else:\n",
    "#                                 noPredictions += 1\n",
    "#                                 print(\"!NO PREDICTIONS!\")                                 \n",
    "                        \n",
    "                # printing average loss / epoch\n",
    "                num_imgs = len(image)\n",
    "                running_loss += loss*num_imgs\n",
    "                running_total += num_imgs\n",
    "                \n",
    "                if p == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "            epoch_loss = float(running_loss/running_total)\n",
    "#             best_score = (sum(bestScore)/len(bestScore))\n",
    "            epoch_acc = (sum(maxIOUs)/len(maxIOUs))\n",
    "            # keep epoch loss / accuracy (for validation)\n",
    "            if p == 'train':\n",
    "                epochLossTrain.append(epoch_loss)\n",
    "#                 epochAccTrain.append(epoch_acc)\n",
    "            if p == 'validate':\n",
    "                \n",
    "                print(len(bestScore), bestScore)\n",
    "                print(len(list_max), list_max)\n",
    "                print(sum(list_max)/len(list_max))\n",
    "                epoch_acc = sum(list_max)/len(list_max)\n",
    "#                 epochLossValidate.append(epoch_loss)\n",
    "                epochAccValidate.append(epoch_acc)\n",
    "#                 epochAccBestScore.append(best_score)\n",
    "            if verbose or (i%10 == 0):\n",
    "                if p == 'train':\n",
    "                    print('Phase:{}, epoch loss: {:.4f}'.format(p, epoch_loss))\n",
    "                if p == 'validate':\n",
    "                    print('Phase:{}, epoch loss: {:.4f} | average best iou: {:.4f}'.format(p, epoch_loss, epoch_acc))\n",
    "#                     print('Phase:{}, epoch loss: {:.4f} | average best iou: {:.4f} | average highest score iou: {:.4f}'.format(p, epoch_loss, epoch_acc, best_score))\n",
    "            # old code\n",
    "#             acc_dict[p].append(epoch_acc)\n",
    "#             loss_dict[p].append(epoch_loss)      \n",
    "            if p == 'validate':\n",
    "                num += 0\n",
    "#                 if epoch_acc > best_acc:\n",
    "#                     best_acc = epoch_acc\n",
    "#                     best_model_wts = model.state_dict()\n",
    "            else:\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "#     model.load_state_dict(best_model_wts)\n",
    "#     return model, acc_dict, loss_dict, epochLossTrain, epochLossValidate, epochAccTrain, epochAccValidate\n",
    "    return model, acc_dict, loss_dict, epochLossTrain, epochAccValidate, epochAccBestScore\n",
    "#     return model, acc_dict, loss_dict, epochLossTrain ,epochLossValidate, epochAccValidate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, acc_dict, loss_dict, epochLossTrain,epochAccValidate, epochAccBestScore = train_model(model, dataloader, optimizer, \n",
    "                                                                        scheduler, \n",
    "                                                                        cel, \n",
    "                                                                        num_epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochLossTrain, label = \"Epoch Train Loss\")\n",
    "# plt.plot(epochLossValidate, label = \"Epoch Validate Loss\")\n",
    "plt.legend(loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(epochLossTrain, label = \"Epoch Train Loss\")\n",
    "plt.plot(epochAccBestScore, label = \"Highest Score IOU\")\n",
    "plt.plot(epochAccValidate, label = \"Max IOU\")\n",
    "plt.legend(loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "# for index, row in test_data.iterrows():\n",
    "for images, targets in (dataloader['test']):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        images = move_to(images,device)\n",
    "        pred = model(images) # Pass the image to the model\n",
    "\n",
    "\n",
    "\n",
    "        for i, out in enumerate(pred):\n",
    "            pred_boxes = [[i[0], i[1], i[2], i[3]] for i in list(out['boxes'].cpu().detach().numpy())]\n",
    "            pred_score = list(out['scores'].cpu().detach().numpy())\n",
    "            pred_class = [classes[i] for i in list(out['labels'].cpu().numpy())]\n",
    "\n",
    "            max_score_index = pred_score.index(max(pred_score))\n",
    "            max_score = pred_boxes[max_score_index]\n",
    "            \n",
    "            list_box = []\n",
    "            for lab in targets[i]['boxes'].numpy():\n",
    "                list_box.append(lab[0]) #xmin\n",
    "                list_box.append(lab[1]) #ymin\n",
    "                list_box.append(lab[2]) #xmax\n",
    "                list_box.append(lab[3]) #ymax\n",
    "            for ind in pred_boxes:\n",
    "                if (bb_intersection_over_union((list_box),(ind)) > .25):\n",
    "                        count += 1\n",
    "                        print(\"prediction box area: \", (ind[2] - ind[0]) * (ind[3] - ind[1]), \"box area: \", (list_box[2] - list_box[0]) * (list_box[3] - list_box[1]))\n",
    "                        print(\"highest score:\" ,max(pred_score), \" min: \", min(pred_score))\n",
    "                        print(\"predicted score:\" , pred_score[i], \"iou:\", bb_intersection_over_union((list_box),(ind)))\n",
    "                        print(pred_score)\n",
    "                        fig,ax = plt.subplots(1, figsize=(5,5))\n",
    "                        ax.imshow(images[i][1].cpu().detach().squeeze().numpy(), cmap='gray')\n",
    "                        label = patches.Rectangle((list_box[0], list_box[1] ),(list_box[2] - list_box[0]),(list_box[3] - list_box[1]),linewidth=1,edgecolor='r',facecolor='none')\n",
    "                        predicted = patches.Rectangle((ind[0], ind[1] ),(ind[2] - ind[0]),(ind[3] - ind[1]),linewidth=1,edgecolor='b',facecolor='none')\n",
    "                        maxScore = patches.Rectangle((max_score[0], max_score[1] ),(max_score[2] - max_score[0]),(max_score[3] - max_score[1]),linewidth=1,edgecolor='y',facecolor='none')\n",
    "                        ax.add_patch(label)\n",
    "                        ax.add_patch(predicted)\n",
    "                        ax.add_patch(maxScore)\n",
    "                        plt.show()\n",
    "    \n",
    "print(count, len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
