{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from skimage import io\n",
    "import time\n",
    "import ast \n",
    "from PIL import *\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random \n",
    "\n",
    "# import dicom \n",
    "import pydicom as dicom\n",
    "import scipy.ndimage\n",
    "\n",
    "from skimage import measure, morphology\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>imageName</th>\n",
       "      <th>SOPInstanceUID</th>\n",
       "      <th>boxes</th>\n",
       "      <th>areas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...</td>\n",
       "      <td>000075.dcm</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.953341886624...</td>\n",
       "      <td>[[161.0, 137.0, 201.0, 162.0]]</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>/scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...</td>\n",
       "      <td>000172.dcm</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.358253510231...</td>\n",
       "      <td>[[337.0, 189.0, 376.0, 224.0]]</td>\n",
       "      <td>1365.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>/scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...</td>\n",
       "      <td>000174.dcm</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.332771657417...</td>\n",
       "      <td>[[335.0, 186.0, 375.0, 225.0]]</td>\n",
       "      <td>1560.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>/scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...</td>\n",
       "      <td>000019.dcm</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.133221309761...</td>\n",
       "      <td>[[333.0, 189.0, 369.0, 219.0]]</td>\n",
       "      <td>1080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>/scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...</td>\n",
       "      <td>000175.dcm</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.291261298826...</td>\n",
       "      <td>[[334.0, 189.0, 374.0, 218.0]]</td>\n",
       "      <td>1160.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path   imageName  \\\n",
       "7   /scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...  000075.dcm   \n",
       "78  /scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...  000172.dcm   \n",
       "79  /scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...  000174.dcm   \n",
       "80  /scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...  000019.dcm   \n",
       "82  /scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...  000175.dcm   \n",
       "\n",
       "                                       SOPInstanceUID  \\\n",
       "7   1.3.6.1.4.1.14519.5.2.1.6279.6001.953341886624...   \n",
       "78  1.3.6.1.4.1.14519.5.2.1.6279.6001.358253510231...   \n",
       "79  1.3.6.1.4.1.14519.5.2.1.6279.6001.332771657417...   \n",
       "80  1.3.6.1.4.1.14519.5.2.1.6279.6001.133221309761...   \n",
       "82  1.3.6.1.4.1.14519.5.2.1.6279.6001.291261298826...   \n",
       "\n",
       "                             boxes   areas  \n",
       "7   [[161.0, 137.0, 201.0, 162.0]]  1000.0  \n",
       "78  [[337.0, 189.0, 376.0, 224.0]]  1365.0  \n",
       "79  [[335.0, 186.0, 375.0, 225.0]]  1560.0  \n",
       "80  [[333.0, 189.0, 369.0, 219.0]]  1080.0  \n",
       "82  [[334.0, 189.0, 374.0, 218.0]]  1160.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"large_nodules.csv\")\n",
    "\n",
    "# filter the size. \n",
    "df = df[df['areas'] >= 1000]  \n",
    "\n",
    "# check cols are \"path\", \"imageName\", \"SOPInstanceUID\", \"boxes\", \"areas\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset aspect ratio range: 0.5471698113207547 2.4827586206896552\n",
      "dataset area range: 1000.0 2880.0\n",
      "anchor area range:  1000.0 2924.0\n"
     ]
    }
   ],
   "source": [
    "# aspect ratio of anchors\n",
    "aspectRatio = []\n",
    "for index, row in df.iterrows():\n",
    "    boxes = ast.literal_eval(row.boxes)\n",
    "    width = float(boxes[0][2]) - float(boxes[0][0])\n",
    "    height = float(boxes[0][3]) - float(boxes[0][1])\n",
    "    aspectRatio.append(width / height)\n",
    "print(\"dataset aspect ratio range:\", min(aspectRatio), max(aspectRatio))\n",
    "\n",
    "# max / min of area\n",
    "print(\"dataset area range:\", min(df['areas']), max(df['areas']))\n",
    "\n",
    "# area > 1000\n",
    "scales = [32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54]\n",
    "aspect_ratios = [0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5]\n",
    "\n",
    "# area > 100\n",
    "# scales = list(range(10, 54))\n",
    "# s = [10, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52]\n",
    "# aspect_ratios = [0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.25, 3.50]\n",
    "calculate_scales = torch.as_tensor(scales)\n",
    "calculate_aspect_ratios = torch.as_tensor(aspect_ratios)\n",
    "h_ratios = torch.sqrt(calculate_aspect_ratios)\n",
    "w_ratios = 1 / h_ratios\n",
    "\n",
    "ws = (w_ratios[:, None] * calculate_scales[None, :]).view(-1)\n",
    "hs = (h_ratios[:, None] * calculate_scales[None, :]).view(-1)\n",
    "\n",
    "base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2\n",
    "base_anchors = (base_anchors.round())\n",
    "anchorArea = []\n",
    "for each in base_anchors:\n",
    "    each = each.numpy()\n",
    "    width = each[2] * 2\n",
    "    height = each[3] * 2\n",
    "    area = width * height\n",
    "    anchorArea.append(area)\n",
    "#     print(each, \" area: \", area)\n",
    "    \n",
    "print(\"anchor area range: \", min(anchorArea), max(anchorArea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into 60, 20, 20 \n",
    "train_data, test_data = train_test_split(df, test_size=0.40, random_state=2020)\n",
    "valid_data, test_data = train_test_split(test_data, test_size=0.50, random_state=2020)\n",
    "train_data.index = np.arange(len(train_data))\n",
    "valid_data.index = np.arange(len(valid_data))\n",
    "test_data.index = np.arange(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make into new csv files\n",
    "train_data.to_csv(\"large_nodules_train.csv\", index=False)\n",
    "valid_data.to_csv(\"large_nodules_valid.csv\", index=False)\n",
    "test_data.to_csv(\"large_nodules_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for preprocessing dicom images - modified for 2d images.\n",
    "# https://www.kaggle.com/akh64bit/full-preprocessing-tutorial\n",
    "def load_scan(path):\n",
    "    slices = [dicom.read_file(path)]\n",
    "    return slices\n",
    "\n",
    "def get_pixels_hu(scans):\n",
    "    image = np.stack([s.pixel_array for s in scans])\n",
    "    # Convert to int16 (from sometimes int16), \n",
    "    # should be possible as values should always be low enough (<32k)\n",
    "    image = image.astype(np.int16)\n",
    "\n",
    "    # Set outside-of-scan pixels to 0\n",
    "    # The intercept is usually -1024, so air is approximately 0\n",
    "    image[image == -2000] = 0\n",
    "    \n",
    "    # Convert to Hounsfield units (HU)\n",
    "    intercept = scans[0].RescaleIntercept\n",
    "    slope = scans[0].RescaleSlope\n",
    "    \n",
    "    if slope != 1:\n",
    "        image = slope * image.astype(np.float64)\n",
    "        image = image.astype(np.int16)\n",
    "        \n",
    "    image += np.int16(intercept)\n",
    "    \n",
    "    return np.array(image, dtype=np.int16)\n",
    "\n",
    "def resample(image, scan, new_spacing=[1,1,1]):\n",
    "    # Determine current pixel spacing\n",
    "#     spacing = map(float, ([scan[0].SliceThickness] + scan[0].PixelSpacing))\n",
    "    spacing = map(float, ([scan[0].SliceThickness] + list(scan[0].PixelSpacing)))\n",
    "    spacing = np.array(list(spacing))\n",
    "    # not 3d\n",
    "    spacing = spacing[1:]\n",
    "\n",
    "    resize_factor = spacing / new_spacing\n",
    "    new_real_shape = image.shape * resize_factor\n",
    "    new_shape = np.round(new_real_shape)\n",
    "    real_resize_factor = new_shape / image.shape\n",
    "    new_spacing = spacing / real_resize_factor\n",
    "    \n",
    "    image = scipy.ndimage.interpolation.zoom(image, real_resize_factor)\n",
    "    \n",
    "    return image, new_spacing\n",
    "\n",
    "def largest_label_volume(im, bg=-1):\n",
    "    vals, counts = np.unique(im, return_counts=True)\n",
    "\n",
    "    counts = counts[vals != bg]\n",
    "    vals = vals[vals != bg]\n",
    "\n",
    "    if len(counts) > 0:\n",
    "        return vals[np.argmax(counts)]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def segment_lung_mask(image, fill_lung_structures=True):\n",
    "    \n",
    "    # not actually binary, but 1 and 2. \n",
    "    # 0 is treated as background, which we do not want\n",
    "    binary_image = np.array(image > -320, dtype=np.int8)+1\n",
    "    labels = measure.label(binary_image)\n",
    "    \n",
    "    # Pick the pixel in the very corner to determine which label is air.\n",
    "    #   Improvement: Pick multiple background labels from around the patient\n",
    "    #   More resistant to \"trays\" on which the patient lays cutting the air \n",
    "    #   around the person in half\n",
    "    \n",
    "#     background_label = labels[0,0,0]\n",
    "    background_label = labels[0,0]\n",
    "    \n",
    "    #Fill the air around the person\n",
    "    binary_image[background_label == labels] = 2\n",
    "    \n",
    "    \n",
    "    # Method of filling the lung structures (that is superior to something like \n",
    "    # morphological closing)\n",
    "    if fill_lung_structures:\n",
    "        # For every slice we determine the largest solid structure\n",
    "        for i, axial_slice in enumerate(binary_image):\n",
    "            axial_slice = axial_slice - 1\n",
    "            labeling = measure.label(axial_slice)\n",
    "            l_max = largest_label_volume(labeling, bg=0)\n",
    "            \n",
    "            if l_max is not None: #This slice contains some lung\n",
    "                binary_image[i][labeling != l_max] = 1\n",
    "\n",
    "    \n",
    "    binary_image -= 1 #Make the image actual binary\n",
    "    binary_image = 1-binary_image # Invert it, lungs are now 1\n",
    "    \n",
    "    # Remove other air pockets insided body\n",
    "    labels = measure.label(binary_image, background=0)\n",
    "#     l_max = largest_label_volume(labels, bg=0)\n",
    "#     if l_max is not None: # There are air pockets\n",
    "#         binary_image[labels != l_max] = 0\n",
    " \n",
    "    return binary_image\n",
    "\n",
    "MIN_BOUND = -1000.0\n",
    "MAX_BOUND = 400.0\n",
    "    \n",
    "def normalize(image):\n",
    "    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND) * 255.\n",
    "    image[image>255] = 255.\n",
    "    image[image<0] = 0.\n",
    "    return image\n",
    "\n",
    "# calculated previously.\n",
    "PIXEL_MEAN = 0.32\n",
    "# PIXEL_MEAN = 0.39\n",
    "\n",
    "def zero_center(image):\n",
    "    image = image - PIXEL_MEAN\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom transforms\n",
    "class RandomHorizontalFlip(object):\n",
    "\n",
    "    \"\"\"Randomly horizontally flips the Image with the probability *p*\n",
    "    Parameters\n",
    "    ----------\n",
    "    p: float\n",
    "        The probability with which the image is flipped\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndaaray\n",
    "        Flipped image in the numpy format of shape `HxWxC`\n",
    "    numpy.ndarray\n",
    "        Tranformed bounding box co-ordinates of the format `n x 4` where n is\n",
    "        number of bounding boxes and 4 represents `x1,y1,x2,y2` of the box\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img, bboxes = sample['image'], sample['boxes']\n",
    "        img = np.array(img)\n",
    "        if random.random() < self.p:\n",
    "            img = img[::-1]\n",
    "            ymax = (256-bboxes[0][1]) + 256 #ymin\n",
    "            ymin = (256-bboxes[0][3]) + 256 #ymax\n",
    "#             fig,ax = plt.subplots(1, figsize=(5,5))\n",
    "#             ax.imshow(img, cmap=\"gray\")\n",
    "#             rect = patches.Rectangle((bboxes[0][0], bboxes[0][1]),(bboxes[0][2] - bboxes[0][0]),(bboxes[0][3] - bboxes[0][1]),linewidth=1,edgecolor='r',facecolor='none')\n",
    "#             ax.add_patch(rect)\n",
    "#             new = patches.Rectangle((bboxes[0][0], ymin),(bboxes[0][2] - bboxes[0][0]),(ymax - ymin),linewidth=1,edgecolor='b',facecolor='none')\n",
    "#             ax.add_patch(new)\n",
    "#             plt.show()\n",
    "            bboxes[0][1] = ymin\n",
    "            bboxes[0][3] = ymax\n",
    "\n",
    "        return {'image': (img),\n",
    "                'boxes': (bboxes)}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, boxes = sample['image'], sample['boxes']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "#         image = image.transpose((2, 0, 1))\n",
    "        toTen = transforms.ToTensor()\n",
    "        image = toTen(image.copy())\n",
    "        return {'image': (image),\n",
    "                'boxes': (boxes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "# torchvision models are trained on input images normalized to [0 1] range .ToPILImage() function achives this\n",
    "# additional normalization is required see: http://pytorch.org/docs/master/torchvision/models.html\n",
    "\n",
    "# no training data augmentation yet\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((512,512),interpolation=Image.NEAREST)]) #scale image up to 512,512\n",
    "#         transforms.ToTensor()])\n",
    "\n",
    "composed = transforms.Compose([RandomHorizontalFlip(),\n",
    "                               ToTensor()])\n",
    "\n",
    "validation_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((512,512),interpolation=Image.NEAREST),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((512,512),interpolation=Image.NEAREST),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "class Faster_RCNN_Dataloader(Dataset):\n",
    "    \"\"\"Chest X-ray dataset from https://nihcc.app.box.com/v/ChestXray-NIHCC.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file filename information.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # get image path\n",
    "        img_name = (self.data_frame.iloc[idx, 0])\n",
    "        \n",
    "        #dicom image preprocessing\n",
    "        scan = load_scan(img_name)\n",
    "        scan_pixels = get_pixels_hu(scan)\n",
    "        pix_resampled, spacing = resample(scan_pixels[0], scan, [1,1])\n",
    "\n",
    "        normal = normalize(pix_resampled)\n",
    "        avg = np.mean(normal, axis=(0, 1))\n",
    "        image = zero_center(normal)\n",
    "        \n",
    "        segmented_lungs = segment_lung_mask(pix_resampled, False)\n",
    "#         print(segmented_lungs)\n",
    "        d = {}\n",
    "    \n",
    "#         image = (image - image.min())/(image.max() - image.min()) * 255.0\n",
    "#         print('normalized: Min: %.3f, Max: %.3f' % (image.min(), image.max()))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(np.uint8(image))\n",
    "            if (self.train):\n",
    "                boxes = torch.FloatTensor(ast.literal_eval(self.data_frame.iloc[idx, 3]))\n",
    "                sample = {'image': image, 'boxes': boxes}\n",
    "                transformed = composed(sample)\n",
    "                image = transformed['image']\n",
    "                d['boxes'] = transformed['boxes']\n",
    "                \n",
    "            else:\n",
    "                d['boxes'] = torch.FloatTensor(ast.literal_eval(self.data_frame.iloc[idx, 3]))\n",
    "\n",
    "        d['labels'] = torch.ones([1], dtype=torch.int64)\n",
    "        d['masks'] = mask_transform(torch.as_tensor(segmented_lungs, dtype=torch.uint8))\n",
    "#         print(mask_transform(d['masks']).shape)\n",
    "#         d[\"image_id\"] = torch.as_tensor(self.data_frame.iloc[idx, 2])\n",
    "        d['area'] = torch.as_tensor(self.data_frame.iloc[idx, 4])\n",
    "        d[\"iscrowd\"] = torch.zeros([1], dtype=torch.int64)\n",
    "\n",
    "        # 1 channel to 3 channel\n",
    "        image = image.numpy()\n",
    "        image = np.stack((image,)*3, axis=-1)\n",
    "        image = image[0]\n",
    "#         print(len(image), len(image[0]), len(image[0][0]), len(image[0][0]))\n",
    "        image = np.transpose(image,(2,0,1))\n",
    "#         print(len(image), len(image[0]), len(image[0][0]))\n",
    "        image = torch.FloatTensor(image)\n",
    "#         image = torch.permute(2, 0, 1) \n",
    "        return image, d\n",
    "\n",
    "# change dataloader output to lists.\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    return list(xx), list(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_path = 'large_nodules_train.csv'\n",
    "validate_df_path = 'large_nodules_valid.csv'\n",
    "test_df_path = 'large_nodules_test.csv'\n",
    "\n",
    "transformed_dataset = {'train': Faster_RCNN_Dataloader(train_df_path, transform=train_transform, train=True),\n",
    "                       'validate':Faster_RCNN_Dataloader(validate_df_path, transform=validation_transform, train=False),\n",
    "                       'test':Faster_RCNN_Dataloader(test_df_path, transform=validation_transform, train=False)}\n",
    "bs = 4\n",
    "\n",
    "dataloader = {x: DataLoader(transformed_dataset[x], batch_size=bs,\n",
    "                        shuffle=True, collate_fn = pad_collate, num_workers=0) for x in ['train', 'validate','test']}\n",
    "data_sizes ={x: len(transformed_dataset[x]) for x in ['train', 'validate','test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print (count, name)\n",
    "#     count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import source_code.mask_rcnn\n",
    "from source_code.faster_rcnn import FastRCNNPredictor\n",
    "from source_code.mask_rcnn import MaskRCNNPredictor\n",
    "from source_code.mask_rcnn import MaskRCNN\n",
    "from source_code.faster_rcnn import FasterRCNN\n",
    "from source_code.rpn import AnchorGenerator\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "#     model = source_code.mask_rcnn.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "\n",
    "    backbone = torchvision.models.vgg16(pretrained=True).features\n",
    "    backbone.out_channels = 512\n",
    "\n",
    "    anchor_generator = AnchorGenerator(sizes=((scales),),\n",
    "                                   aspect_ratios=((aspect_ratios),))\n",
    "\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[\"0\"],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2) \n",
    "    model = MaskRCNN(backbone,\n",
    "           num_classes=2,\n",
    "           rpn_anchor_generator=anchor_generator,\n",
    "           box_roi_pool=roi_pooler).to(device)\n",
    "    \n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "    model = MaskRCNN(backbone,\n",
    "               num_classes=2,\n",
    "               rpn_anchor_generator=anchor_generator,\n",
    "               box_roi_pool=roi_pooler).to(device)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneralizedRCNNTransform(\n",
       "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "    Resize(min_size=(512,), max_size=512, mode='bilinear')\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model_instance_segmentation(2) \n",
    "model = model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "model.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(512,), max_size=512, mode='bilinear')\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(512, 108, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(512, 432, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "    (mask_roi_pool): MultiScaleRoIAlign()\n",
       "    (mask_head): MaskRCNNHeads(\n",
       "      (mask_fcn1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu2): ReLU(inplace=True)\n",
       "      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu3): ReLU(inplace=True)\n",
       "      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu4): ReLU(inplace=True)\n",
       "    )\n",
       "    (mask_predictor): MaskRCNNPredictor(\n",
       "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask_fcn_logits): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['__background__', 'nodule']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(float(boxA[0]), float(boxB[0]))\n",
    "    yA = max(float(boxA[1]), float(boxB[1]))\n",
    "    xB = min(float(boxA[2]), float(boxB[2]))\n",
    "    yB = min(float(boxA[3]), float(boxB[3]))\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have loss / accuracy in case training is interrupted\n",
    "outsideLoss = []\n",
    "outsideValidLoss = []\n",
    "outsideAcc = []\n",
    "outsideIOU = []\n",
    "\n",
    "def train_model(model, dataloader,optimizer, scheduler, iou_threshold = 0, num_epochs = 10, verbose = True):\n",
    "    phases = ['train','validate']\n",
    "    since = time.time()\n",
    "    best_acc = 0\n",
    "    epochLossTrain = list() # loss for train\n",
    "    epochAccTrain = list() \n",
    "    epochLossValidate = list()\n",
    "    epochAccValidate = list() # how many images' highest confidence score has an iou > 0\n",
    "    epochAccIOU = list()\n",
    "    running_correct = 0\n",
    "    running_loss = 0\n",
    "    running_total = 0\n",
    "    for i in range(num_epochs):\n",
    "        print('Epoch: {}/{}'.format(i, num_epochs-1))\n",
    "        print('-'*10)\n",
    "        losslist = []\n",
    "        for p in phases:\n",
    "            if p == 'train':\n",
    "                model.train()\n",
    "                totalImg = 0\n",
    "                validateAcc = 0\n",
    "                IOUAvg = []\n",
    "            else:\n",
    "                model.eval()   \n",
    "                validateAcc = 0\n",
    "                totalImg = 0\n",
    "                IOUAvg = []\n",
    "            num = 0 # calculate which batch # --> enumerate not working\n",
    "            for image, target in dataloader[p]:\n",
    "                image = [img.to(device) for img in image]\n",
    "                target = [{k: v.to(device) for k, v in t.items()} for t in target]\n",
    "                \n",
    "\n",
    "                if (num % int(len(dataloader[p])/4) == 0):\n",
    "                    print('{} set | epoch: {:3d} | {:6d}/{:6d} batches'.format(p, i, num, len(dataloader[p])))\n",
    "                num = num + 1\n",
    "                if p == 'train':\n",
    "                    # loss = sum of all 4 losses returned\n",
    "                    lossDict, output = model((image), target)\n",
    "                    loss = sum(loss for loss in lossDict.values())\n",
    "            \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "#                     train accuracy\n",
    "                    pred_score = [] # confidence score\n",
    "                    pred_boxes = [] # bounding box\n",
    "                    pred_class = [] # predicted class\n",
    "#                     loop through output (len(output) = batch size)\n",
    "                    for j, out in enumerate(output):\n",
    "                        totalImg += 1 \n",
    "                        \n",
    "                        # documentation: ['boxes'] = [x1, y1, x2, y2]\n",
    "                        pred_boxes = [[i[0], i[1], i[2], i[3]] for i in list(out['boxes'].cpu().detach().numpy())]\n",
    "                        pred_score = list(out['scores'].cpu().detach().numpy())\n",
    "                        pred_class = [classes[i] for i in list(out['labels'].cpu().numpy())]\n",
    "                            \n",
    "                        # get labeled  bounding box\n",
    "                        # documentation: # 0 xmin, 1 ymin, 2 xmax, 3 ymax\n",
    "                        t = target[j]['boxes'].cpu().detach().numpy()[0]\n",
    "                        target_box =  [t[0],t[1],t[2],t[3]]\n",
    "                        \n",
    "                        if len(pred_score) > 0:\n",
    "                            max_score_index = pred_score.index(max(pred_score))\n",
    "                            max_score_IOU = bb_intersection_over_union(target_box, pred_boxes[max_score_index])\n",
    "                            # calculate how many top confidence has iou > 0\n",
    "                            if (max_score_IOU > iou_threshold):\n",
    "                                validateAcc += 1\n",
    "                                IOUAvg.append(max_score_IOU)\n",
    "                                \n",
    "                if p == 'validate':\n",
    "                    # helps with memory \n",
    "                    with torch.no_grad():\n",
    "                        model.eval()  \n",
    "                        lossDict, output = model((image), target)\n",
    "                        loss = sum(loss for loss in lossDict.values())\n",
    "        \n",
    "                        pred_score = [] # confidence score\n",
    "                        pred_boxes = [] # bounding box\n",
    "                        pred_class = [] # predicted class\n",
    "                        # loop through output (len(output) = batch size)\n",
    "                        for j, out in enumerate(output):\n",
    "                        \n",
    "                            totalImg += 1 \n",
    "                            \n",
    "                            # documentation: ['boxes'] = [x1, y1, x2, y2]\n",
    "                            pred_boxes = [[i[0], i[1], i[2], i[3]] for i in list(out['boxes'].cpu().detach().numpy())]\n",
    "                            pred_score = list(out['scores'].cpu().detach().numpy())\n",
    "                            pred_class = [classes[i] for i in list(out['labels'].cpu().numpy())]\n",
    "                            \n",
    "                            # get labeled  bounding box\n",
    "                            # documentation: # 0 xmin, 1 ymin, 2 xmax, 3 ymax\n",
    "                            t = target[j]['boxes'].cpu().detach().numpy()[0]\n",
    "                            target_box =  [t[0],t[1],t[2],t[3]]\n",
    "                            if len(pred_score) > 0:\n",
    "                                max_score_index = pred_score.index(max(pred_score))\n",
    "                                max_score_IOU = bb_intersection_over_union(target_box, pred_boxes[max_score_index])\n",
    "\n",
    "                                # calculate how many top confidence has iou > 0\n",
    "                                if (max_score_IOU > iou_threshold):\n",
    "                                    validateAcc += 1\n",
    "                                    IOUAvg.append(max_score_IOU)\n",
    "                                \n",
    "                            #maybe to calculate map\n",
    "                            tempS = []\n",
    "#                             for s, score in enumerate(pred_score):\n",
    "#                                 if (score > 0.6):\n",
    "# #                                     print(bb_intersection_over_union(target_box, pred_boxes[s]))\n",
    "#                                     tempS.append(bb_intersection_over_union(target_box, pred_boxes[s]))\n",
    "# #                             (sum(tempS)/len(tempS)\n",
    "#                             print(max(pred_score), min(pred_score))\n",
    "                # printing average loss / epoch\n",
    "                num_imgs = len(image)\n",
    "                running_loss += loss*num_imgs\n",
    "                running_total += num_imgs  \n",
    "            # keep epoch loss / accuracy (for validation)\n",
    "            if p == 'train':\n",
    "                epoch_loss = float(running_loss/running_total)\n",
    "                epochLossTrain.append(epoch_loss)\n",
    "                \n",
    "                outsideLoss.append(epoch_loss) # in case model get cancelled \n",
    "                \n",
    "                epoch_acc = (validateAcc/totalImg) \n",
    "                epochAccTrain.append(epoch_acc)\n",
    "            if p == 'validate':\n",
    "                epoch_loss = float(running_loss/running_total)\n",
    "                epochLossValidate.append(epoch_loss)\n",
    "                outsideValidLoss.append(epoch_loss) \n",
    "                    \n",
    "                epoch_acc = (validateAcc/totalImg) * 100\n",
    "                if (len(IOUAvg) > 0):\n",
    "#                     print(IOUAvg)\n",
    "                    avgIOU = sum(IOUAvg)/len(IOUAvg)\n",
    "                else: \n",
    "                    avgIOU = 0\n",
    "                epochAccValidate.append(epoch_acc)\n",
    "                epochAccIOU.append(avgIOU)\n",
    "        \n",
    "                outsideAcc.append(epoch_acc)\n",
    "                outsideIOU.append(avgIOU)\n",
    "            if verbose or (i%10 == 0):\n",
    "                if p == 'train':\n",
    "#                     print('Phase:{}, epoch loss: {:.4f}'.format(p, epoch_loss))\n",
    "                    print('Phase:{}, epoch loss: {:.4f} | epoch accuracy: {:.4f} '.format(p, epoch_loss, epoch_acc))\n",
    "                if p == 'validate':\n",
    "                    print('Phase:{}, epoch loss: {:.4f} | epoch accuracy: {:.4f} | average of (correctly predicted) iou {:.4f}'.format(p, epoch_loss, epoch_acc, avgIOU))   \n",
    "#                     print('Phase:{},  average accuracy: {:.4f} | average of (correctly predicted) iou {:.4f}'.format(p, epoch_acc, avgIOU))   \n",
    "                \n",
    "            if p == 'validate':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = model.state_dict()\n",
    "            \n",
    "            # LambdaLR scheduler\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "                    # ReduceLROnPlateau\n",
    "#                     scheduler.step(epoch_loss)\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "#     return model, epochLossTrain, epochLossValidate, epochAccValidate, epochAccIOU\n",
    "    return model, epochLossTrain, epochAccTrain, epochLossValidate  ,epochAccValidate, epochAccIOU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/49\n",
      "----------\n",
      "train set | epoch:   0 |      0/    72 batches\n",
      "train set | epoch:   0 |     18/    72 batches\n",
      "train set | epoch:   0 |     36/    72 batches\n",
      "train set | epoch:   0 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.5582 | epoch accuracy: 0.0000 \n",
      "validate set | epoch:   0 |      0/    24 batches\n",
      "validate set | epoch:   0 |      6/    24 batches\n",
      "validate set | epoch:   0 |     12/    24 batches\n",
      "validate set | epoch:   0 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.4454 | epoch accuracy: 0.0000 | average of (correctly predicted) iou 0.0000\n",
      "Epoch: 1/49\n",
      "----------\n",
      "train set | epoch:   1 |      0/    72 batches\n",
      "train set | epoch:   1 |     18/    72 batches\n",
      "train set | epoch:   1 |     36/    72 batches\n",
      "train set | epoch:   1 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.3199 | epoch accuracy: 0.0350 \n",
      "validate set | epoch:   1 |      0/    24 batches\n",
      "validate set | epoch:   1 |      6/    24 batches\n",
      "validate set | epoch:   1 |     12/    24 batches\n",
      "validate set | epoch:   1 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.2950 | epoch accuracy: 3.1250 | average of (correctly predicted) iou 0.6163\n",
      "Epoch: 2/49\n",
      "----------\n",
      "train set | epoch:   2 |      0/    72 batches\n",
      "train set | epoch:   2 |     18/    72 batches\n",
      "train set | epoch:   2 |     36/    72 batches\n",
      "train set | epoch:   2 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.2547 | epoch accuracy: 0.1469 \n",
      "validate set | epoch:   2 |      0/    24 batches\n",
      "validate set | epoch:   2 |      6/    24 batches\n",
      "validate set | epoch:   2 |     12/    24 batches\n",
      "validate set | epoch:   2 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.2436 | epoch accuracy: 23.9583 | average of (correctly predicted) iou 0.8146\n",
      "Epoch: 3/49\n",
      "----------\n",
      "train set | epoch:   3 |      0/    72 batches\n",
      "train set | epoch:   3 |     18/    72 batches\n",
      "train set | epoch:   3 |     36/    72 batches\n",
      "train set | epoch:   3 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.2239 | epoch accuracy: 0.2622 \n",
      "validate set | epoch:   3 |      0/    24 batches\n",
      "validate set | epoch:   3 |      6/    24 batches\n",
      "validate set | epoch:   3 |     12/    24 batches\n",
      "validate set | epoch:   3 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.2178 | epoch accuracy: 32.2917 | average of (correctly predicted) iou 0.8257\n",
      "Epoch: 4/49\n",
      "----------\n",
      "train set | epoch:   4 |      0/    72 batches\n",
      "train set | epoch:   4 |     18/    72 batches\n",
      "train set | epoch:   4 |     36/    72 batches\n",
      "train set | epoch:   4 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.2054 | epoch accuracy: 0.3217 \n",
      "validate set | epoch:   4 |      0/    24 batches\n",
      "validate set | epoch:   4 |      6/    24 batches\n",
      "validate set | epoch:   4 |     12/    24 batches\n",
      "validate set | epoch:   4 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.2013 | epoch accuracy: 35.4167 | average of (correctly predicted) iou 0.8070\n",
      "Epoch: 5/49\n",
      "----------\n",
      "train set | epoch:   5 |      0/    72 batches\n",
      "train set | epoch:   5 |     18/    72 batches\n",
      "train set | epoch:   5 |     36/    72 batches\n",
      "train set | epoch:   5 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1926 | epoch accuracy: 0.3392 \n",
      "validate set | epoch:   5 |      0/    24 batches\n",
      "validate set | epoch:   5 |      6/    24 batches\n",
      "validate set | epoch:   5 |     12/    24 batches\n",
      "validate set | epoch:   5 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1898 | epoch accuracy: 36.4583 | average of (correctly predicted) iou 0.8418\n",
      "Epoch: 6/49\n",
      "----------\n",
      "train set | epoch:   6 |      0/    72 batches\n",
      "train set | epoch:   6 |     18/    72 batches\n",
      "train set | epoch:   6 |     36/    72 batches\n",
      "train set | epoch:   6 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1839 | epoch accuracy: 0.3287 \n",
      "validate set | epoch:   6 |      0/    24 batches\n",
      "validate set | epoch:   6 |      6/    24 batches\n",
      "validate set | epoch:   6 |     12/    24 batches\n",
      "validate set | epoch:   6 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1818 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8224\n",
      "Epoch: 7/49\n",
      "----------\n",
      "train set | epoch:   7 |      0/    72 batches\n",
      "train set | epoch:   7 |     18/    72 batches\n",
      "train set | epoch:   7 |     36/    72 batches\n",
      "train set | epoch:   7 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1774 | epoch accuracy: 0.3776 \n",
      "validate set | epoch:   7 |      0/    24 batches\n",
      "validate set | epoch:   7 |      6/    24 batches\n",
      "validate set | epoch:   7 |     12/    24 batches\n",
      "validate set | epoch:   7 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1757 | epoch accuracy: 39.5833 | average of (correctly predicted) iou 0.8261\n",
      "Epoch: 8/49\n",
      "----------\n",
      "train set | epoch:   8 |      0/    72 batches\n",
      "train set | epoch:   8 |     18/    72 batches\n",
      "train set | epoch:   8 |     36/    72 batches\n",
      "train set | epoch:   8 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1721 | epoch accuracy: 0.3776 \n",
      "validate set | epoch:   8 |      0/    24 batches\n",
      "validate set | epoch:   8 |      6/    24 batches\n",
      "validate set | epoch:   8 |     12/    24 batches\n",
      "validate set | epoch:   8 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1707 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8213\n",
      "Epoch: 9/49\n",
      "----------\n",
      "train set | epoch:   9 |      0/    72 batches\n",
      "train set | epoch:   9 |     18/    72 batches\n",
      "train set | epoch:   9 |     36/    72 batches\n",
      "train set | epoch:   9 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1680 | epoch accuracy: 0.4126 \n",
      "validate set | epoch:   9 |      0/    24 batches\n",
      "validate set | epoch:   9 |      6/    24 batches\n",
      "validate set | epoch:   9 |     12/    24 batches\n",
      "validate set | epoch:   9 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1669 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8213\n",
      "Epoch: 10/49\n",
      "----------\n",
      "train set | epoch:  10 |      0/    72 batches\n",
      "train set | epoch:  10 |     18/    72 batches\n",
      "train set | epoch:  10 |     36/    72 batches\n",
      "train set | epoch:  10 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1646 | epoch accuracy: 0.3706 \n",
      "validate set | epoch:  10 |      0/    24 batches\n",
      "validate set | epoch:  10 |      6/    24 batches\n",
      "validate set | epoch:  10 |     12/    24 batches\n",
      "validate set | epoch:  10 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1637 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 11/49\n",
      "----------\n",
      "train set | epoch:  11 |      0/    72 batches\n",
      "train set | epoch:  11 |     18/    72 batches\n",
      "train set | epoch:  11 |     36/    72 batches\n",
      "train set | epoch:  11 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1619 | epoch accuracy: 0.3776 \n",
      "validate set | epoch:  11 |      0/    24 batches\n",
      "validate set | epoch:  11 |      6/    24 batches\n",
      "validate set | epoch:  11 |     12/    24 batches\n",
      "validate set | epoch:  11 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1611 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 12/49\n",
      "----------\n",
      "train set | epoch:  12 |      0/    72 batches\n",
      "train set | epoch:  12 |     18/    72 batches\n",
      "train set | epoch:  12 |     36/    72 batches\n",
      "train set | epoch:  12 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1595 | epoch accuracy: 0.3706 \n",
      "validate set | epoch:  12 |      0/    24 batches\n",
      "validate set | epoch:  12 |      6/    24 batches\n",
      "validate set | epoch:  12 |     12/    24 batches\n",
      "validate set | epoch:  12 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1588 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 13/49\n",
      "----------\n",
      "train set | epoch:  13 |      0/    72 batches\n",
      "train set | epoch:  13 |     18/    72 batches\n",
      "train set | epoch:  13 |     36/    72 batches\n",
      "train set | epoch:  13 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1575 | epoch accuracy: 0.3706 \n",
      "validate set | epoch:  13 |      0/    24 batches\n",
      "validate set | epoch:  13 |      6/    24 batches\n",
      "validate set | epoch:  13 |     12/    24 batches\n",
      "validate set | epoch:  13 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1569 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 14/49\n",
      "----------\n",
      "train set | epoch:  14 |      0/    72 batches\n",
      "train set | epoch:  14 |     18/    72 batches\n",
      "train set | epoch:  14 |     36/    72 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set | epoch:  14 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1558 | epoch accuracy: 0.3671 \n",
      "validate set | epoch:  14 |      0/    24 batches\n",
      "validate set | epoch:  14 |      6/    24 batches\n",
      "validate set | epoch:  14 |     12/    24 batches\n",
      "validate set | epoch:  14 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1553 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 15/49\n",
      "----------\n",
      "train set | epoch:  15 |      0/    72 batches\n",
      "train set | epoch:  15 |     18/    72 batches\n",
      "train set | epoch:  15 |     36/    72 batches\n",
      "train set | epoch:  15 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1542 | epoch accuracy: 0.3566 \n",
      "validate set | epoch:  15 |      0/    24 batches\n",
      "validate set | epoch:  15 |      6/    24 batches\n",
      "validate set | epoch:  15 |     12/    24 batches\n",
      "validate set | epoch:  15 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1538 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 16/49\n",
      "----------\n",
      "train set | epoch:  16 |      0/    72 batches\n",
      "train set | epoch:  16 |     18/    72 batches\n",
      "train set | epoch:  16 |     36/    72 batches\n",
      "train set | epoch:  16 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1529 | epoch accuracy: 0.3846 \n",
      "validate set | epoch:  16 |      0/    24 batches\n",
      "validate set | epoch:  16 |      6/    24 batches\n",
      "validate set | epoch:  16 |     12/    24 batches\n",
      "validate set | epoch:  16 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1525 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 17/49\n",
      "----------\n",
      "train set | epoch:  17 |      0/    72 batches\n",
      "train set | epoch:  17 |     18/    72 batches\n",
      "train set | epoch:  17 |     36/    72 batches\n",
      "train set | epoch:  17 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1516 | epoch accuracy: 0.3881 \n",
      "validate set | epoch:  17 |      0/    24 batches\n",
      "validate set | epoch:  17 |      6/    24 batches\n",
      "validate set | epoch:  17 |     12/    24 batches\n",
      "validate set | epoch:  17 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1512 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 18/49\n",
      "----------\n",
      "train set | epoch:  18 |      0/    72 batches\n",
      "train set | epoch:  18 |     18/    72 batches\n",
      "train set | epoch:  18 |     36/    72 batches\n",
      "train set | epoch:  18 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1506 | epoch accuracy: 0.3671 \n",
      "validate set | epoch:  18 |      0/    24 batches\n",
      "validate set | epoch:  18 |      6/    24 batches\n",
      "validate set | epoch:  18 |     12/    24 batches\n",
      "validate set | epoch:  18 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1503 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 19/49\n",
      "----------\n",
      "train set | epoch:  19 |      0/    72 batches\n",
      "train set | epoch:  19 |     18/    72 batches\n",
      "train set | epoch:  19 |     36/    72 batches\n",
      "train set | epoch:  19 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1497 | epoch accuracy: 0.3601 \n",
      "validate set | epoch:  19 |      0/    24 batches\n",
      "validate set | epoch:  19 |      6/    24 batches\n",
      "validate set | epoch:  19 |     12/    24 batches\n",
      "validate set | epoch:  19 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1494 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 20/49\n",
      "----------\n",
      "train set | epoch:  20 |      0/    72 batches\n",
      "train set | epoch:  20 |     18/    72 batches\n",
      "train set | epoch:  20 |     36/    72 batches\n",
      "train set | epoch:  20 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1489 | epoch accuracy: 0.3566 \n",
      "validate set | epoch:  20 |      0/    24 batches\n",
      "validate set | epoch:  20 |      6/    24 batches\n",
      "validate set | epoch:  20 |     12/    24 batches\n",
      "validate set | epoch:  20 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1486 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 21/49\n",
      "----------\n",
      "train set | epoch:  21 |      0/    72 batches\n",
      "train set | epoch:  21 |     18/    72 batches\n",
      "train set | epoch:  21 |     36/    72 batches\n",
      "train set | epoch:  21 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1481 | epoch accuracy: 0.3916 \n",
      "validate set | epoch:  21 |      0/    24 batches\n",
      "validate set | epoch:  21 |      6/    24 batches\n",
      "validate set | epoch:  21 |     12/    24 batches\n",
      "validate set | epoch:  21 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1479 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 22/49\n",
      "----------\n",
      "train set | epoch:  22 |      0/    72 batches\n",
      "train set | epoch:  22 |     18/    72 batches\n",
      "train set | epoch:  22 |     36/    72 batches\n",
      "train set | epoch:  22 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1475 | epoch accuracy: 0.3811 \n",
      "validate set | epoch:  22 |      0/    24 batches\n",
      "validate set | epoch:  22 |      6/    24 batches\n",
      "validate set | epoch:  22 |     12/    24 batches\n",
      "validate set | epoch:  22 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1472 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 23/49\n",
      "----------\n",
      "train set | epoch:  23 |      0/    72 batches\n",
      "train set | epoch:  23 |     18/    72 batches\n",
      "train set | epoch:  23 |     36/    72 batches\n",
      "train set | epoch:  23 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1468 | epoch accuracy: 0.3776 \n",
      "validate set | epoch:  23 |      0/    24 batches\n",
      "validate set | epoch:  23 |      6/    24 batches\n",
      "validate set | epoch:  23 |     12/    24 batches\n",
      "validate set | epoch:  23 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1466 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 24/49\n",
      "----------\n",
      "train set | epoch:  24 |      0/    72 batches\n",
      "train set | epoch:  24 |     18/    72 batches\n",
      "train set | epoch:  24 |     36/    72 batches\n",
      "train set | epoch:  24 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1462 | epoch accuracy: 0.3811 \n",
      "validate set | epoch:  24 |      0/    24 batches\n",
      "validate set | epoch:  24 |      6/    24 batches\n",
      "validate set | epoch:  24 |     12/    24 batches\n",
      "validate set | epoch:  24 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1460 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 25/49\n",
      "----------\n",
      "train set | epoch:  25 |      0/    72 batches\n",
      "train set | epoch:  25 |     18/    72 batches\n",
      "train set | epoch:  25 |     36/    72 batches\n",
      "train set | epoch:  25 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1456 | epoch accuracy: 0.3776 \n",
      "validate set | epoch:  25 |      0/    24 batches\n",
      "validate set | epoch:  25 |      6/    24 batches\n",
      "validate set | epoch:  25 |     12/    24 batches\n",
      "validate set | epoch:  25 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1454 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 26/49\n",
      "----------\n",
      "train set | epoch:  26 |      0/    72 batches\n",
      "train set | epoch:  26 |     18/    72 batches\n",
      "train set | epoch:  26 |     36/    72 batches\n",
      "train set | epoch:  26 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1451 | epoch accuracy: 0.4056 \n",
      "validate set | epoch:  26 |      0/    24 batches\n",
      "validate set | epoch:  26 |      6/    24 batches\n",
      "validate set | epoch:  26 |     12/    24 batches\n",
      "validate set | epoch:  26 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1449 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 27/49\n",
      "----------\n",
      "train set | epoch:  27 |      0/    72 batches\n",
      "train set | epoch:  27 |     18/    72 batches\n",
      "train set | epoch:  27 |     36/    72 batches\n",
      "train set | epoch:  27 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1446 | epoch accuracy: 0.3776 \n",
      "validate set | epoch:  27 |      0/    24 batches\n",
      "validate set | epoch:  27 |      6/    24 batches\n",
      "validate set | epoch:  27 |     12/    24 batches\n",
      "validate set | epoch:  27 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1444 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 28/49\n",
      "----------\n",
      "train set | epoch:  28 |      0/    72 batches\n",
      "train set | epoch:  28 |     18/    72 batches\n",
      "train set | epoch:  28 |     36/    72 batches\n",
      "train set | epoch:  28 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1442 | epoch accuracy: 0.3566 \n",
      "validate set | epoch:  28 |      0/    24 batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate set | epoch:  28 |      6/    24 batches\n",
      "validate set | epoch:  28 |     12/    24 batches\n",
      "validate set | epoch:  28 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1440 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 29/49\n",
      "----------\n",
      "train set | epoch:  29 |      0/    72 batches\n",
      "train set | epoch:  29 |     18/    72 batches\n",
      "train set | epoch:  29 |     36/    72 batches\n",
      "train set | epoch:  29 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1438 | epoch accuracy: 0.3427 \n",
      "validate set | epoch:  29 |      0/    24 batches\n",
      "validate set | epoch:  29 |      6/    24 batches\n",
      "validate set | epoch:  29 |     12/    24 batches\n",
      "validate set | epoch:  29 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1436 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 30/49\n",
      "----------\n",
      "train set | epoch:  30 |      0/    72 batches\n",
      "train set | epoch:  30 |     18/    72 batches\n",
      "train set | epoch:  30 |     36/    72 batches\n",
      "train set | epoch:  30 |     54/    72 batches\n",
      "Phase:train, epoch loss: 0.1434 | epoch accuracy: 0.3846 \n",
      "validate set | epoch:  30 |      0/    24 batches\n",
      "validate set | epoch:  30 |      6/    24 batches\n",
      "validate set | epoch:  30 |     12/    24 batches\n",
      "validate set | epoch:  30 |     18/    24 batches\n",
      "Phase:validate, epoch loss: 0.1432 | epoch accuracy: 40.6250 | average of (correctly predicted) iou 0.8212\n",
      "Epoch: 31/49\n",
      "----------\n",
      "train set | epoch:  31 |      0/    72 batches\n"
     ]
    }
   ],
   "source": [
    "model, epochLossTrain, epochAccTrain, epochLossValidate, epochAccValidate, epochAccIOU = train_model(model, \n",
    "                                                                        dataloader, optimizer, \n",
    "                                                                        scheduler,\n",
    "                                                                        iou_threshold = 0.5,\n",
    "                                                                        num_epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b0094e89610>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochLossTrain, label = \"Epoch Train Loss\")\n",
    "plt.plot(epochLossValidate, label = \"Epoch Validation Loss\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "\n",
    "# if model gets cancelled, plot:\n",
    "# plt.plot(outsideLoss, label = \"Epoch Train Loss\")\n",
    "# plt.plot(outsideValidLoss, label = \"Epoch Validation Loss\")\n",
    "# plt.legend(loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b00a1592ed0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(epochAccTrain, label = \"Train Accuracy\")\n",
    "plt.plot(epochAccValidate, label = \"Validation Accuracy\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "\n",
    "# if model gets cancelled, plot:\n",
    "# plt.plot(outsideAcc, label = \"Accuracy of Model\")\n",
    "# plt.legend(loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochAccIOU, label = \"Average of (Correctly Predicted) IOU\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "         \n",
    "# plt.plot(outsideIOU, label = \"Average of (Correctly Predicted) IOU\")\n",
    "# plt.legend(loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "somePrediction = 0 \n",
    "for images, targets in (dataloader['test']):\n",
    "#     print(targets)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        image = [img.to(device) for img in image]\n",
    "        target = [{k: v.to(device) for k, v in t.items()} for t in target]\n",
    "#         print(modelTargets)\n",
    "        loss, pred = model(images, targets) # Pass the image to the model\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        total += 1\n",
    "        list_box = []\n",
    "        for lab in targets[i]['boxes'].cpu().detach().numpy():\n",
    "            ymin, ymax = lab[1], lab[3] #ymin ymax        \n",
    "            xmin, xmax = lab[0], lab[2] #xmin, xmax\n",
    "#             print(xmin, ymin, xmax, ymax)\n",
    "            list_box.append(lab[0]) #xmin\n",
    "            list_box.append(lab[1]) #ymin\n",
    "            list_box.append(lab[2]) #xmax\n",
    "            list_box.append(lab[3]) #ymax\n",
    "            \n",
    "        out = pred[i]\n",
    "        pred_boxes = [[i[0], i[1], i[2], i[3]] for i in list(out['boxes'].cpu().detach().numpy())]\n",
    "        pred_score = list(out['scores'].cpu().detach().numpy())\n",
    "        pred_class = [classes[i] for i in list(out['labels'].cpu().numpy())]\n",
    "\n",
    "        max_score_index = pred_score.index(max(pred_score))\n",
    "        max_score = pred_boxes[max_score_index]\n",
    "        \n",
    "        if (bb_intersection_over_union((list_box),max_score) > 0):\n",
    "            count += 1\n",
    "            print(\"iou: \", bb_intersection_over_union((list_box),max_score))\n",
    "            print(\"prediction box area: \", (max_score[2] - max_score[0]) * (max_score[3] - max_score[1]), \"box area: \", (list_box[2] - list_box[0]) * (list_box[3] - list_box[1]))\n",
    "            fig,ax = plt.subplots(1, figsize=(5,5))\n",
    "            ax.imshow(img[0].cpu().squeeze().numpy(), cmap=\"gray\")\n",
    "#             label = patches.Rectangle((, list_box[1] ),(list_box[2] - list_box[0]),(list_box[3] - list_box[1]),linewidth=1,edgecolor='r',facecolor='none')\n",
    "            label = patches.Rectangle((xmin, ymin ),(xmax - xmin),(ymax - ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "            maxScore = patches.Rectangle((max_score[0], max_score[1] ),(max_score[2] - max_score[0]),(max_score[3] - max_score[1]),linewidth=1,edgecolor='y',facecolor='none')\n",
    "            ax.add_patch(label)\n",
    "            ax.add_patch(maxScore)\n",
    "            plt.show()\n",
    "        if_once = False\n",
    "        for i, ind in enumerate(pred_boxes):\n",
    "            if (bb_intersection_over_union((list_box),(ind)) > .2):\n",
    "                if_once = True\n",
    "#                 fig,ax = plt.subplots(1, figsize=(5,5))\n",
    "#                 ax.imshow(img[0].cpu().squeeze().numpy(), cmap=\"gray\")\n",
    "#                 label = patches.Rectangle((xmin, ymin ),(xmax - xmin),(ymax - ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "#                 label = patches.Rectangle((list_box[0], list_box[1] ),(list_box[2] - list_box[0]),(list_box[3] - list_box[1]),linewidth=1,edgecolor='r',facecolor='none')\n",
    "#                 maxScore = patches.Rectangle((max_score[0], max_score[1] ),(max_score[2] - max_score[0]),(max_score[3] - max_score[1]),linewidth=1,edgecolor='y',facecolor='none')\n",
    "#                 prediction = patches.Rectangle((ind[0], ind[1] ),(ind[2] - ind[0]),(ind[3] - ind[1]),linewidth=1,edgecolor='b',facecolor='none')\n",
    "#                 ax.add_patch(label)\n",
    "#                 ax.add_patch(maxScore)\n",
    "#                 ax.add_patch(prediction)\n",
    "#                 plt.show()   \n",
    "#                 print(\"prediction box area: \", (ind[2] - ind[0]) * (ind[3] - ind[1]), \"box area: \", ((list_box[2] - list_box[0]) * (list_box[3] - list_box[1])))\n",
    "#                 print(\"highest score:\" ,max(pred_score), \" min: \", min(pred_score))\n",
    "#                 print(\"predicted score:\" , pred_score[i], \"iou:\", bb_intersection_over_union((list_box),(ind)))\n",
    "#                 print(\"all scores\" , i)\n",
    "        \n",
    "        if if_once:\n",
    "            somePrediction += 1\n",
    "print(count, somePrediction, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
