{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from skimage import io\n",
    "import time\n",
    "import ast \n",
    "from PIL import *\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random \n",
    "\n",
    "# import dicom \n",
    "import pydicom as dicom\n",
    "import scipy.ndimage\n",
    "\n",
    "from skimage import measure, morphology\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>imageName</th>\n",
       "      <th>SOPInstanceUID</th>\n",
       "      <th>boxes</th>\n",
       "      <th>areas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...</td>\n",
       "      <td>000059.dcm</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.319084926500...</td>\n",
       "      <td>[[165.0, 140.0, 199.0, 161.0]]</td>\n",
       "      <td>714.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...</td>\n",
       "      <td>000008.dcm</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.749728215582...</td>\n",
       "      <td>[[167.0, 134.0, 201.0, 160.0]]</td>\n",
       "      <td>884.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...</td>\n",
       "      <td>000109.dcm</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.327906493223...</td>\n",
       "      <td>[[167.0, 134.0, 202.0, 161.0]]</td>\n",
       "      <td>945.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...</td>\n",
       "      <td>000063.dcm</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.230680133701...</td>\n",
       "      <td>[[168.0, 135.0, 201.0, 160.0]]</td>\n",
       "      <td>825.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...</td>\n",
       "      <td>000005.dcm</td>\n",
       "      <td>1.3.6.1.4.1.14519.5.2.1.6279.6001.212532866259...</td>\n",
       "      <td>[[180.0, 133.0, 200.0, 154.0]]</td>\n",
       "      <td>420.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path   imageName  \\\n",
       "0  /scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...  000059.dcm   \n",
       "1  /scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...  000008.dcm   \n",
       "2  /scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...  000109.dcm   \n",
       "3  /scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...  000063.dcm   \n",
       "4  /scratch/ebc308/tcia/data/LIDC-IDRI/LIDC-IDRI-...  000005.dcm   \n",
       "\n",
       "                                      SOPInstanceUID  \\\n",
       "0  1.3.6.1.4.1.14519.5.2.1.6279.6001.319084926500...   \n",
       "1  1.3.6.1.4.1.14519.5.2.1.6279.6001.749728215582...   \n",
       "2  1.3.6.1.4.1.14519.5.2.1.6279.6001.327906493223...   \n",
       "3  1.3.6.1.4.1.14519.5.2.1.6279.6001.230680133701...   \n",
       "4  1.3.6.1.4.1.14519.5.2.1.6279.6001.212532866259...   \n",
       "\n",
       "                            boxes  areas  \n",
       "0  [[165.0, 140.0, 199.0, 161.0]]  714.0  \n",
       "1  [[167.0, 134.0, 201.0, 160.0]]  884.0  \n",
       "2  [[167.0, 134.0, 202.0, 161.0]]  945.0  \n",
       "3  [[168.0, 135.0, 201.0, 160.0]]  825.0  \n",
       "4  [[180.0, 133.0, 200.0, 154.0]]  420.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"large_nodules.csv\")\n",
    "\n",
    "# filter the size. \n",
    "df = df[df['areas'] >= 1000]  \n",
    "\n",
    "# check cols are \"path\", \"imageName\", \"SOPInstanceUID\", \"boxes\", \"areas\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3333333333333335 0.3\n",
      "2880.0 100.0\n",
      "tensor(3008.) tensor(96.)\n"
     ]
    }
   ],
   "source": [
    "# aspect ratio of anchors\n",
    "aspectRatio = []\n",
    "for index, row in df.iterrows():\n",
    "    boxes = ast.literal_eval(row.boxes)\n",
    "    width = float(boxes[0][2]) - float(boxes[0][0])\n",
    "    height = float(boxes[0][3]) - float(boxes[0][1])\n",
    "    aspectRatio.append(width / height)\n",
    "print(max(aspectRatio), min(aspectRatio))\n",
    "\n",
    "\n",
    "# max / min of area\n",
    "print(max(df['areas']), min(df['areas']))\n",
    "\n",
    "# area > 1000\n",
    "scales = [32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54]\n",
    "aspect_ratios = [0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5]\n",
    "\n",
    "# area > 100\n",
    "# scales = list(range(10, 55))\n",
    "# s = [10, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52]\n",
    "# aspect_ratios = [0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.25, 3.50]\n",
    "\n",
    "calculate_scales = torch.as_tensor(scales)\n",
    "calculate_aspect_ratios = torch.as_tensor(aspect_ratios)\n",
    "h_ratios = torch.sqrt(calculate_aspect_ratios)\n",
    "w_ratios = 1 / h_ratios\n",
    "\n",
    "ws = (w_ratios[:, None] * calculate_scales[None, :]).view(-1)\n",
    "hs = (h_ratios[:, None] * calculate_scales[None, :]).view(-1)\n",
    "\n",
    "base_anchors = torch.stack([-ws, -hs, ws, hs], dim=1) / 2\n",
    "base_anchors = (base_anchors.round())\n",
    "anchorArea = []\n",
    "for each in base_anchors:\n",
    "    width = each[2] * 2\n",
    "    height = each[3] * 2\n",
    "    area = width * height\n",
    "    anchorArea.append(area)\n",
    "#     print(each, \" area: \", area)\n",
    "    \n",
    "print(max(anchorArea), min(anchorArea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into 60, 20, 20 \n",
    "train_data, test_data = train_test_split(df, test_size=0.40, random_state=2020)\n",
    "valid_data, test_data = train_test_split(test_data, test_size=0.50, random_state=2020)\n",
    "train_data.index = np.arange(len(train_data))\n",
    "valid_data.index = np.arange(len(valid_data))\n",
    "test_data.index = np.arange(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make into new csv files\n",
    "train_data.to_csv(\"large_nodules_train.csv\", index=False)\n",
    "valid_data.to_csv(\"large_nodules_valid.csv\", index=False)\n",
    "test_data.to_csv(\"large_nodules_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for preprocessing dicom images - modified for 2d images.\n",
    "# https://www.kaggle.com/akh64bit/full-preprocessing-tutorial\n",
    "def load_scan(path):\n",
    "    slices = [dicom.read_file(path)]\n",
    "    return slices\n",
    "\n",
    "def get_pixels_hu(scans):\n",
    "    image = np.stack([s.pixel_array for s in scans])\n",
    "    # Convert to int16 (from sometimes int16), \n",
    "    # should be possible as values should always be low enough (<32k)\n",
    "    image = image.astype(np.int16)\n",
    "\n",
    "    # Set outside-of-scan pixels to 0\n",
    "    # The intercept is usually -1024, so air is approximately 0\n",
    "    image[image == -2000] = 0\n",
    "    \n",
    "    # Convert to Hounsfield units (HU)\n",
    "    intercept = scans[0].RescaleIntercept\n",
    "    slope = scans[0].RescaleSlope\n",
    "    \n",
    "    if slope != 1:\n",
    "        image = slope * image.astype(np.float64)\n",
    "        image = image.astype(np.int16)\n",
    "        \n",
    "    image += np.int16(intercept)\n",
    "    \n",
    "    return np.array(image, dtype=np.int16)\n",
    "\n",
    "def resample(image, scan, new_spacing=[1,1,1]):\n",
    "    # Determine current pixel spacing\n",
    "#     spacing = map(float, ([scan[0].SliceThickness] + scan[0].PixelSpacing))\n",
    "    spacing = map(float, ([scan[0].SliceThickness] + list(scan[0].PixelSpacing)))\n",
    "    spacing = np.array(list(spacing))\n",
    "    # not 3d\n",
    "    spacing = spacing[1:]\n",
    "\n",
    "    resize_factor = spacing / new_spacing\n",
    "    new_real_shape = image.shape * resize_factor\n",
    "    new_shape = np.round(new_real_shape)\n",
    "    real_resize_factor = new_shape / image.shape\n",
    "    new_spacing = spacing / real_resize_factor\n",
    "    \n",
    "    image = scipy.ndimage.interpolation.zoom(image, real_resize_factor)\n",
    "    \n",
    "    return image, new_spacing\n",
    "\n",
    "def largest_label_volume(im, bg=-1):\n",
    "    vals, counts = np.unique(im, return_counts=True)\n",
    "\n",
    "    counts = counts[vals != bg]\n",
    "    vals = vals[vals != bg]\n",
    "\n",
    "    if len(counts) > 0:\n",
    "        return vals[np.argmax(counts)]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def segment_lung_mask(image, fill_lung_structures=True):\n",
    "    \n",
    "    # not actually binary, but 1 and 2. \n",
    "    # 0 is treated as background, which we do not want\n",
    "    binary_image = np.array(image > -320, dtype=np.int8)+1\n",
    "    labels = measure.label(binary_image)\n",
    "    \n",
    "    # Pick the pixel in the very corner to determine which label is air.\n",
    "    #   Improvement: Pick multiple background labels from around the patient\n",
    "    #   More resistant to \"trays\" on which the patient lays cutting the air \n",
    "    #   around the person in half\n",
    "    \n",
    "#     background_label = labels[0,0,0]\n",
    "    background_label = labels[0,0]\n",
    "    \n",
    "    #Fill the air around the person\n",
    "    binary_image[background_label == labels] = 2\n",
    "    \n",
    "    \n",
    "    # Method of filling the lung structures (that is superior to something like \n",
    "    # morphological closing)\n",
    "    if fill_lung_structures:\n",
    "        # For every slice we determine the largest solid structure\n",
    "        for i, axial_slice in enumerate(binary_image):\n",
    "            axial_slice = axial_slice - 1\n",
    "            labeling = measure.label(axial_slice)\n",
    "            l_max = largest_label_volume(labeling, bg=0)\n",
    "            \n",
    "            if l_max is not None: #This slice contains some lung\n",
    "                binary_image[i][labeling != l_max] = 1\n",
    "\n",
    "    \n",
    "    binary_image -= 1 #Make the image actual binary\n",
    "    binary_image = 1-binary_image # Invert it, lungs are now 1\n",
    "    \n",
    "    # Remove other air pockets insided body\n",
    "    labels = measure.label(binary_image, background=0)\n",
    "#     l_max = largest_label_volume(labels, bg=0)\n",
    "#     if l_max is not None: # There are air pockets\n",
    "#         binary_image[labels != l_max] = 0\n",
    " \n",
    "    return binary_image\n",
    "\n",
    "MIN_BOUND = -1000.0\n",
    "MAX_BOUND = 400.0\n",
    "    \n",
    "def normalize(image):\n",
    "    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND) * 255.\n",
    "    image[image>255] = 255.\n",
    "    image[image<0] = 0.\n",
    "    return image\n",
    "\n",
    "# calculated previously.\n",
    "PIXEL_MEAN = 0.32\n",
    "# PIXEL_MEAN = 0.39\n",
    "\n",
    "def zero_center(image):\n",
    "    image = image - PIXEL_MEAN\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom transforms\n",
    "class RandomHorizontalFlip(object):\n",
    "\n",
    "    \"\"\"Randomly horizontally flips the Image with the probability *p*\n",
    "    Parameters\n",
    "    ----------\n",
    "    p: float\n",
    "        The probability with which the image is flipped\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndaaray\n",
    "        Flipped image in the numpy format of shape `HxWxC`\n",
    "    numpy.ndarray\n",
    "        Tranformed bounding box co-ordinates of the format `n x 4` where n is\n",
    "        number of bounding boxes and 4 represents `x1,y1,x2,y2` of the box\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img, bboxes = sample['image'], sample['boxes']\n",
    "        img = np.array(img)\n",
    "        if random.random() < self.p:\n",
    "            img = img[::-1]\n",
    "            ymax = (256-bboxes[0][1]) + 256 #ymin\n",
    "            ymin = (256-bboxes[0][3]) + 256 #ymax\n",
    "#             fig,ax = plt.subplots(1, figsize=(5,5))\n",
    "#             ax.imshow(img, cmap=\"gray\")\n",
    "#             rect = patches.Rectangle((bboxes[0][0], bboxes[0][1]),(bboxes[0][2] - bboxes[0][0]),(bboxes[0][3] - bboxes[0][1]),linewidth=1,edgecolor='r',facecolor='none')\n",
    "#             ax.add_patch(rect)\n",
    "#             new = patches.Rectangle((bboxes[0][0], ymin),(bboxes[0][2] - bboxes[0][0]),(ymax - ymin),linewidth=1,edgecolor='b',facecolor='none')\n",
    "#             ax.add_patch(new)\n",
    "#             plt.show()\n",
    "            bboxes[0][1] = ymin\n",
    "            bboxes[0][3] = ymax\n",
    "\n",
    "        return {'image': (img),\n",
    "                'boxes': (bboxes)}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, boxes = sample['image'], sample['boxes']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "#         image = image.transpose((2, 0, 1))\n",
    "        toTen = transforms.ToTensor()\n",
    "        image = toTen(image.copy())\n",
    "        return {'image': (image),\n",
    "                'boxes': (boxes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "# torchvision models are trained on input images normalized to [0 1] range .ToPILImage() function achives this\n",
    "# additional normalization is required see: http://pytorch.org/docs/master/torchvision/models.html\n",
    "\n",
    "# no training data augmentation yet\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((512,512),interpolation=Image.NEAREST)]) #scale image up to 512,512\n",
    "#         transforms.ToTensor()])\n",
    "\n",
    "composed = transforms.Compose([RandomHorizontalFlip(),\n",
    "                               ToTensor()])\n",
    "\n",
    "validation_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((512,512),interpolation=Image.NEAREST),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "class Faster_RCNN_Dataloader(Dataset):\n",
    "    \"\"\"Chest X-ray dataset from https://nihcc.app.box.com/v/ChestXray-NIHCC.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file filename information.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # get image path\n",
    "        img_name = (self.data_frame.iloc[idx, 0])\n",
    "        \n",
    "        #dicom image preprocessing\n",
    "        scan = load_scan(img_name)\n",
    "        scan_pixels = get_pixels_hu(scan)\n",
    "        pix_resampled, spacing = resample(scan_pixels[0], scan, [1,1])\n",
    "        segmented_lungs = segment_lung_mask(pix_resampled, False)\n",
    "        normal = normalize(segmented_lungs)\n",
    "        avg = np.mean(normal, axis=(0, 1))\n",
    "        image = zero_center(normal)\n",
    "        d = {}\n",
    "    \n",
    "#         image = (image - image.min())/(image.max() - image.min()) * 255.0\n",
    "#         print('normalized: Min: %.3f, Max: %.3f' % (image.min(), image.max()))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(np.uint8(image))\n",
    "            if (self.train):\n",
    "                boxes = torch.FloatTensor(ast.literal_eval(self.data_frame.iloc[idx, 3]))\n",
    "                sample = {'image': image, 'boxes': boxes}\n",
    "                transformed = composed(sample)\n",
    "                image = transformed['image']\n",
    "                d['boxes'] = transformed['boxes']\n",
    "                d['labels'] = torch.ones([1], dtype=torch.int64)\n",
    "            else:\n",
    "                d['boxes'] = torch.FloatTensor(ast.literal_eval(self.data_frame.iloc[idx, 3]))\n",
    "                d['labels'] = torch.ones([1], dtype=torch.int64)\n",
    "\n",
    "        \n",
    "        return image, d\n",
    "\n",
    "# change dataloader output to lists.\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    return list(xx), list(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_path = 'large_nodules_train.csv'\n",
    "validate_df_path = 'large_nodules_valid.csv'\n",
    "test_df_path = 'large_nodules_test.csv'\n",
    "\n",
    "transformed_dataset = {'train': Faster_RCNN_Dataloader(train_df_path, transform=train_transform, train=True),\n",
    "                       'validate':Faster_RCNN_Dataloader(validate_df_path, transform=validation_transform, train=False),\n",
    "                       'test':Faster_RCNN_Dataloader(test_df_path, transform=validation_transform, train=False)}\n",
    "bs = 2\n",
    "\n",
    "dataloader = {x: DataLoader(transformed_dataset[x], batch_size=bs,\n",
    "                        shuffle=True, collate_fn = pad_collate, num_workers=0) for x in ['train', 'validate','test']}\n",
    "data_sizes ={x: len(transformed_dataset[x]) for x in ['train', 'validate','test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if dataloader loaded correctly\n",
    "images1, targets1 = next(iter(dataloader['train']))\n",
    "images2, targets2 = next(iter(dataloader['validate']))\n",
    "images3, targets3 = next(iter(dataloader['test']))\n",
    "\n",
    "# to check dim of list dataloader\n",
    "# len(images3) # = batch\n",
    "# len(images3[0]) # = channel\n",
    "# len(images3[0][0]) # = height\n",
    "# len(images3[4][0][0]) # = width\n",
    "\n",
    "# print only one image from one batch\n",
    "\n",
    "# images4, targets4 = next(iter(dataloader['train']))\n",
    "# # print(len(images4))\n",
    "# for i in targets4[0]['boxes'].numpy():\n",
    "#     ymin, ymax = i[1],i[3] #ymin ymax        \n",
    "#     xmin, xmax = i[0], i[2] #xmin, xmax\n",
    "# print(xmin,ymin,xmax,ymax)\n",
    "# fig,ax = plt.subplots(1, figsize=(5,5))\n",
    "# ax.imshow(images4[0].squeeze().numpy(), cmap='gray')\n",
    "# rect = patches.Rectangle((xmin, ymin ),(xmax - xmin),(ymax - ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "# ax.add_patch(rect)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# print all images from dataloader\n",
    "\n",
    "# for images, targets in (dataloader['train']):\n",
    "#     for i, img in enumerate(images):\n",
    "#         for i in targets[i]['boxes'].numpy():\n",
    "#             ymin, ymax = i[1],i[3] #ymin ymax        \n",
    "#             xmin, xmax = i[0], i[2] #xmin, xmax\n",
    "#             fig,ax = plt.subplots(1, figsize=(5,5))\n",
    "#             ax.imshow(img[0].squeeze().numpy(), cmap=\"gray\")\n",
    "#             rect = patches.Rectangle((xmin, ymin ),(xmax - xmin),(ymax - ymin),linewidth=1,edgecolor='r',facecolor='none')\n",
    "#             ax.add_patch(rect)\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Freeze model weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (nodule) + background\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.vgg16(pretrained=True).features\n",
    "\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For vgg16, it's 512\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 512\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "\n",
    "# anchor_generator = AnchorGenerator(sizes=((10, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52),),\n",
    "#                                    aspect_ratios=((0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 2.75, 3.0, 3.25, 3.50),))\n",
    "\n",
    "# areas > 1000\n",
    "# scales and aspect_ratios defined at the top chunk.\n",
    "anchor_generator = AnchorGenerator(sizes=((scales),),\n",
    "                                   aspect_ratios=((aspect_ratios),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[\"0\"],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler).to(device) # \n",
    "\n",
    "# tried to run not pretrained.\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(num_classes=2).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-50)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.00001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "lambda_func = lambda epoch: 0.5 ** epoch\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_func) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(512, 585, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(512, 2340, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move dict/lists to gpu \n",
    "def move_to(obj, device):\n",
    "    if torch.is_tensor(obj):\n",
    "        return obj.to(device)\n",
    "    elif isinstance(obj, dict):\n",
    "        res = {}\n",
    "        for k, v in obj.items():\n",
    "            res[k] = move_to(v, device)\n",
    "        return res\n",
    "    elif isinstance(obj, list):\n",
    "        res = []\n",
    "        for v in obj:\n",
    "            res.append(move_to(v, device))\n",
    "        return res\n",
    "    else:\n",
    "        raise TypeError(\"Invalid type for move_to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['__background__', 'nodule']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(float(boxA[0]), float(boxB[0]))\n",
    "    yA = max(float(boxA[1]), float(boxB[1]))\n",
    "    xB = min(float(boxA[2]), float(boxB[2]))\n",
    "    yB = min(float(boxA[3]), float(boxB[3]))\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have loss / accuracy in case training is interrupted\n",
    "outsideLoss = []\n",
    "outsideAcc = []\n",
    "outsideIOU = []\n",
    "\n",
    "def train_model(model, dataloader,optimizer, scheduler, iou_threshold = 0, num_epochs = 10, verbose = True):\n",
    "    phases = ['train','validate']\n",
    "    since = time.time()\n",
    "    best_acc = 0\n",
    "    epochLossTrain = list() # loss for train\n",
    "    epochAccValidate = list() # how many images' highest confidence score has an iou > 0\n",
    "    epochAccIOU = list()\n",
    "    for i in range(num_epochs):\n",
    "        print('Epoch: {}/{}'.format(i, num_epochs-1))\n",
    "        print('-'*10)\n",
    "        losslist = []\n",
    "        for p in phases:\n",
    "            if p == 'train':\n",
    "                running_correct = 0\n",
    "                running_loss = 0\n",
    "                running_total = 0\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()   \n",
    "                validateAcc = 0\n",
    "                totalImg = 0\n",
    "                IOUAvg = []\n",
    "            num = 0 # calculate which batch # --> enumerate not working\n",
    "            for image, target in dataloader[p]:\n",
    "#                 optimizer.zero_grad()\n",
    "                image = move_to(image,device)\n",
    "                target = move_to(target,device)\n",
    "#                 print(target)\n",
    "                if (num % int(len(dataloader[p])/4) == 0):\n",
    "                    print('{} set | epoch: {:3d} | {:6d}/{:6d} batches'.format(p, i, num, len(dataloader[p])))\n",
    "                num = num + 1\n",
    "                if p == 'train':\n",
    "                    # loss = sum of all 4 losses returned\n",
    "                    output = model((image), target)\n",
    "                    loss = sum(loss for loss in output.values())\n",
    "#                     print(loss)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # printing average loss / epoch\n",
    "                    num_imgs = len(image)\n",
    "                    running_loss += loss*num_imgs\n",
    "                    running_total += num_imgs\n",
    "                if p == 'validate':\n",
    "                    # helps with memory \n",
    "                    with torch.no_grad():\n",
    "                        model.eval()  \n",
    "                        output = model((image))\n",
    "                        pred_score = [] # confidence score\n",
    "                        pred_boxes = [] # bounding box\n",
    "                        pred_class = [] # predicted class\n",
    "                        # loop through output (len(output) = batch size)\n",
    "                        for j, out in enumerate(output):\n",
    "                            totalImg += 1 \n",
    "                            \n",
    "                            # documentation: ['boxes'] = [x1, y1, x2, y2]\n",
    "                            pred_boxes = [[i[0], i[1], i[2], i[3]] for i in list(out['boxes'].cpu().detach().numpy())]\n",
    "                            pred_score = list(out['scores'].cpu().detach().numpy())\n",
    "                            pred_class = [classes[i] for i in list(out['labels'].cpu().numpy())]\n",
    "                            \n",
    "                            # get labeled  bounding box\n",
    "                            # documentation: # 0 xmin, 1 ymin, 2 xmax, 3 ymax\n",
    "                            t = target[j]['boxes'].cpu().detach().numpy()[0]\n",
    "                            target_box =  [t[0],t[1],t[2],t[3]]\n",
    "                            \n",
    "                            max_score_index = pred_score.index(max(pred_score))\n",
    "                            max_score_IOU = bb_intersection_over_union(target_box, pred_boxes[max_score_index])\n",
    "                            # calculate how many top confidence has iou > 0\n",
    "                            if (max_score_IOU > iou_threshold):\n",
    "                                validateAcc += 1\n",
    "                                IOUAvg.append(max_score_IOU)\n",
    "                    \n",
    "            # keep epoch loss / accuracy (for validation)\n",
    "            if p == 'train':\n",
    "                epoch_loss = float(running_loss/running_total)\n",
    "                epochLossTrain.append(epoch_loss)\n",
    "                \n",
    "                outsideLoss.append(epoch_loss) # in case model get cancelled \n",
    "            if p == 'validate':\n",
    "                epoch_acc = (validateAcc/totalImg) * 100.0\n",
    "                if (len(IOUAvg) > 0):\n",
    "                    avgIOU = sum(IOUAvg)/len(IOUAvg)\n",
    "                else: \n",
    "                    avgIOU = 0\n",
    "                epochAccValidate.append(epoch_acc)\n",
    "                epochAccIOU.append(avgIOU)\n",
    "        \n",
    "                outsideAcc.append(epoch_acc)\n",
    "                outsideIOU.append(avgIOU)\n",
    "            if verbose or (i%10 == 0):\n",
    "                if p == 'train':\n",
    "                    print('Phase:{}, epoch loss: {:.4f}'.format(p, epoch_loss))\n",
    "                if p == 'validate':\n",
    "                    print('Phase:{}, average accuracy: {:.4f} | average of (correctly predicted) iou {:.4f}'.format(p, epoch_acc, avgIOU))   \n",
    "            if p == 'validate':\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = model.state_dict()\n",
    "            else:\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, epochLossTrain, epochAccValidate, epochAccIOU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/9\n",
      "----------\n",
      "train set | epoch:   0 |      0/  1561 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/njm363/deeplearning/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set | epoch:   0 |    390/  1561 batches\n",
      "train set | epoch:   0 |    780/  1561 batches\n",
      "train set | epoch:   0 |   1170/  1561 batches\n",
      "train set | epoch:   0 |   1560/  1561 batches\n",
      "Phase:train, epoch loss: 1.3869\n",
      "validate set | epoch:   0 |      0/   521 batches\n",
      "validate set | epoch:   0 |    130/   521 batches\n",
      "validate set | epoch:   0 |    260/   521 batches\n",
      "validate set | epoch:   0 |    390/   521 batches\n",
      "validate set | epoch:   0 |    520/   521 batches\n",
      "Phase:validate, average accuracy: 0.1921 | average of (correctly predicted) iou 0.0928\n",
      "Epoch: 1/9\n",
      "----------\n",
      "train set | epoch:   1 |      0/  1561 batches\n",
      "train set | epoch:   1 |    390/  1561 batches\n",
      "train set | epoch:   1 |    780/  1561 batches\n",
      "train set | epoch:   1 |   1170/  1561 batches\n",
      "train set | epoch:   1 |   1560/  1561 batches\n",
      "Phase:train, epoch loss: 1.3464\n",
      "validate set | epoch:   1 |      0/   521 batches\n",
      "validate set | epoch:   1 |    130/   521 batches\n",
      "validate set | epoch:   1 |    260/   521 batches\n",
      "validate set | epoch:   1 |    390/   521 batches\n",
      "validate set | epoch:   1 |    520/   521 batches\n",
      "Phase:validate, average accuracy: 0.0000 | average of (correctly predicted) iou 0.0000\n",
      "Epoch: 2/9\n",
      "----------\n",
      "train set | epoch:   2 |      0/  1561 batches\n",
      "train set | epoch:   2 |    390/  1561 batches\n",
      "train set | epoch:   2 |    780/  1561 batches\n",
      "train set | epoch:   2 |   1170/  1561 batches\n",
      "train set | epoch:   2 |   1560/  1561 batches\n",
      "Phase:train, epoch loss: 1.3248\n",
      "validate set | epoch:   2 |      0/   521 batches\n",
      "validate set | epoch:   2 |    130/   521 batches\n",
      "validate set | epoch:   2 |    260/   521 batches\n",
      "validate set | epoch:   2 |    390/   521 batches\n",
      "validate set | epoch:   2 |    520/   521 batches\n",
      "Phase:validate, average accuracy: 0.0000 | average of (correctly predicted) iou 0.0000\n",
      "Epoch: 3/9\n",
      "----------\n",
      "train set | epoch:   3 |      0/  1561 batches\n",
      "train set | epoch:   3 |    390/  1561 batches\n",
      "train set | epoch:   3 |    780/  1561 batches\n",
      "train set | epoch:   3 |   1170/  1561 batches\n",
      "train set | epoch:   3 |   1560/  1561 batches\n",
      "Phase:train, epoch loss: 1.3133\n",
      "validate set | epoch:   3 |      0/   521 batches\n",
      "validate set | epoch:   3 |    130/   521 batches\n",
      "validate set | epoch:   3 |    260/   521 batches\n",
      "validate set | epoch:   3 |    390/   521 batches\n",
      "validate set | epoch:   3 |    520/   521 batches\n",
      "Phase:validate, average accuracy: 0.0000 | average of (correctly predicted) iou 0.0000\n",
      "Epoch: 4/9\n",
      "----------\n",
      "train set | epoch:   4 |      0/  1561 batches\n",
      "train set | epoch:   4 |    390/  1561 batches\n",
      "train set | epoch:   4 |    780/  1561 batches\n",
      "train set | epoch:   4 |   1170/  1561 batches\n",
      "train set | epoch:   4 |   1560/  1561 batches\n",
      "Phase:train, epoch loss: 1.3074\n",
      "validate set | epoch:   4 |      0/   521 batches\n",
      "validate set | epoch:   4 |    130/   521 batches\n",
      "validate set | epoch:   4 |    260/   521 batches\n",
      "validate set | epoch:   4 |    390/   521 batches\n",
      "validate set | epoch:   4 |    520/   521 batches\n",
      "Phase:validate, average accuracy: 0.0000 | average of (correctly predicted) iou 0.0000\n",
      "Epoch: 5/9\n",
      "----------\n",
      "train set | epoch:   5 |      0/  1561 batches\n",
      "train set | epoch:   5 |    390/  1561 batches\n",
      "train set | epoch:   5 |    780/  1561 batches\n",
      "train set | epoch:   5 |   1170/  1561 batches\n",
      "train set | epoch:   5 |   1560/  1561 batches\n",
      "Phase:train, epoch loss: 1.3045\n",
      "validate set | epoch:   5 |      0/   521 batches\n",
      "validate set | epoch:   5 |    130/   521 batches\n",
      "validate set | epoch:   5 |    260/   521 batches\n",
      "validate set | epoch:   5 |    390/   521 batches\n",
      "validate set | epoch:   5 |    520/   521 batches\n",
      "Phase:validate, average accuracy: 0.0000 | average of (correctly predicted) iou 0.0000\n",
      "Epoch: 6/9\n",
      "----------\n",
      "train set | epoch:   6 |      0/  1561 batches\n",
      "train set | epoch:   6 |    390/  1561 batches\n",
      "train set | epoch:   6 |    780/  1561 batches\n",
      "train set | epoch:   6 |   1170/  1561 batches\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-00f909041545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                                                                         \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                         \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                                                         iou_threshold = 0)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-cc528d93da1f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, scheduler, iou_threshold, num_epochs, verbose)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0;31m# loss = sum of all 4 losses returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m#                     print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib/python3.7/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib/python3.7/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         strides = [[torch.tensor(image_size[0] / g[0], dtype=torch.int64, device=device),\n\u001b[0;32m--> 164\u001b[0;31m                     torch.tensor(image_size[1] / g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_grid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib/python3.7/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         strides = [[torch.tensor(image_size[0] / g[0], dtype=torch.int64, device=device),\n\u001b[0;32m--> 164\u001b[0;31m                     torch.tensor(image_size[1] / g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_grid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, epochLossTrain, epochAccValidate, epochAccIOU = train_model(model, \n",
    "                                                                        dataloader, optimizer, \n",
    "                                                                        scheduler,\n",
    "                                                                        iou_threshold = 0,\n",
    "                                                                        num_epochs = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochLossTrain, label = \"Epoch Train Loss\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "\n",
    "# if model gets cancelled, plot:\n",
    "# plt.plot(outsideLoss, label = \"Epoch Train Loss\")\n",
    "# plt.legend(loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochAccValidate, label = \"Accuracy of Model\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "\n",
    "# if model gets cancelled, plot:\n",
    "# plt.plot(outsideAcc, label = \"Accuracy of Model\")\n",
    "# plt.legend(loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochAccIOU, label = \"Average of (Correctly Predicted) IOU\")\n",
    "plt.legend(loc = \"upper right\")\n",
    "         \n",
    "# plt.plot(outsideIOU, label = \"Average of (Correctly Predicted) IOU\")\n",
    "# plt.legend(loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/njm363/deeplearning/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5124, 0.5114, 0.5109, 0.5109, 0.5101, 0.5099, 0.5099, 0.5099, 0.5098,\n",
      "        0.5098, 0.5086, 0.5082, 0.5082, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5076,\n",
      "        0.5074, 0.5074, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5068, 0.5067, 0.5066, 0.5066, 0.5065, 0.5064, 0.5060, 0.5057, 0.5057,\n",
      "        0.5056, 0.5054, 0.5054, 0.5053, 0.5053, 0.5052, 0.5051, 0.5049, 0.5049,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5124, 0.5114, 0.5109, 0.5109, 0.5101, 0.5099, 0.5099, 0.5099, 0.5098,\n",
      "        0.5098, 0.5086, 0.5082, 0.5082, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5076,\n",
      "        0.5074, 0.5074, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5068, 0.5067, 0.5066, 0.5066, 0.5065, 0.5064, 0.5060, 0.5057, 0.5057,\n",
      "        0.5056, 0.5054, 0.5054, 0.5053, 0.5053, 0.5052, 0.5051, 0.5049, 0.5049,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5123, 0.5113, 0.5108, 0.5108, 0.5100, 0.5099, 0.5099, 0.5098, 0.5098,\n",
      "        0.5098, 0.5097, 0.5096, 0.5096, 0.5096, 0.5094, 0.5092, 0.5089, 0.5089,\n",
      "        0.5088, 0.5087, 0.5087, 0.5086, 0.5085, 0.5084, 0.5084, 0.5083, 0.5083,\n",
      "        0.5083, 0.5083, 0.5083, 0.5082, 0.5082, 0.5082, 0.5082, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079,\n",
      "        0.5079, 0.5078, 0.5074, 0.5074, 0.5073, 0.5072, 0.5072, 0.5072, 0.5071,\n",
      "        0.5071, 0.5071, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5068, 0.5067, 0.5067, 0.5066,\n",
      "        0.5066, 0.5065, 0.5063, 0.5059, 0.5057, 0.5056, 0.5056, 0.5055, 0.5054,\n",
      "        0.5054, 0.5054, 0.5053, 0.5052, 0.5052, 0.5052, 0.5051, 0.5050, 0.5049,\n",
      "        0.5049], device='cuda:0')\n",
      "tensor([0.5131, 0.5118, 0.5117, 0.5115, 0.5111, 0.5107, 0.5107, 0.5106, 0.5106,\n",
      "        0.5104, 0.5103, 0.5103, 0.5102, 0.5102, 0.5101, 0.5100, 0.5100, 0.5099,\n",
      "        0.5098, 0.5097, 0.5096, 0.5095, 0.5095, 0.5091, 0.5090, 0.5089, 0.5089,\n",
      "        0.5089, 0.5089, 0.5087, 0.5087, 0.5086, 0.5086, 0.5085, 0.5084, 0.5083,\n",
      "        0.5083, 0.5083, 0.5082, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5077, 0.5076, 0.5076, 0.5073, 0.5073, 0.5072, 0.5071, 0.5071, 0.5071,\n",
      "        0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5068, 0.5067, 0.5067,\n",
      "        0.5067, 0.5066, 0.5064, 0.5063, 0.5062, 0.5061, 0.5059, 0.5058, 0.5056,\n",
      "        0.5056], device='cuda:0')\n",
      "tensor([0.5166, 0.5146, 0.5144, 0.5142, 0.5130, 0.5129, 0.5128, 0.5118, 0.5116,\n",
      "        0.5113, 0.5113, 0.5111, 0.5106, 0.5102, 0.5101, 0.5101, 0.5099, 0.5099,\n",
      "        0.5097, 0.5093, 0.5090, 0.5089, 0.5089, 0.5088, 0.5088, 0.5088, 0.5088,\n",
      "        0.5088, 0.5087, 0.5086, 0.5086, 0.5083, 0.5083, 0.5083, 0.5083, 0.5083,\n",
      "        0.5082, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082,\n",
      "        0.5082, 0.5082, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081,\n",
      "        0.5081, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5079, 0.5079, 0.5079, 0.5078, 0.5078, 0.5077, 0.5077, 0.5076, 0.5075,\n",
      "        0.5073, 0.5073, 0.5072, 0.5072, 0.5071, 0.5071, 0.5071, 0.5070, 0.5070,\n",
      "        0.5070, 0.5070, 0.5069, 0.5069, 0.5068, 0.5068, 0.5068, 0.5068, 0.5067,\n",
      "        0.5067, 0.5067, 0.5066, 0.5066, 0.5065, 0.5064, 0.5064, 0.5063, 0.5062,\n",
      "        0.5057], device='cuda:0')\n",
      "tensor([0.5120, 0.5115, 0.5109, 0.5107, 0.5105, 0.5105, 0.5104, 0.5104, 0.5104,\n",
      "        0.5104, 0.5102, 0.5101, 0.5101, 0.5100, 0.5100, 0.5100, 0.5100, 0.5099,\n",
      "        0.5099, 0.5099, 0.5099, 0.5097, 0.5097, 0.5096, 0.5096, 0.5095, 0.5095,\n",
      "        0.5095, 0.5095, 0.5094, 0.5094, 0.5094, 0.5092, 0.5092, 0.5091, 0.5090,\n",
      "        0.5090, 0.5089, 0.5089, 0.5089, 0.5089, 0.5088, 0.5088, 0.5087, 0.5087,\n",
      "        0.5086, 0.5086, 0.5086, 0.5085, 0.5085, 0.5084, 0.5084, 0.5084, 0.5084,\n",
      "        0.5084, 0.5083, 0.5083, 0.5083, 0.5083, 0.5083, 0.5082, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5080, 0.5080, 0.5080, 0.5077, 0.5076, 0.5076, 0.5076,\n",
      "        0.5075, 0.5075, 0.5075, 0.5074, 0.5073, 0.5072, 0.5069, 0.5069, 0.5069,\n",
      "        0.5067, 0.5067, 0.5066, 0.5065, 0.5065, 0.5061, 0.5060, 0.5059, 0.5059,\n",
      "        0.5059, 0.5058, 0.5058, 0.5058, 0.5058, 0.5057, 0.5056, 0.5056, 0.5055,\n",
      "        0.5055], device='cuda:0')\n",
      "tensor([0.5123, 0.5113, 0.5108, 0.5108, 0.5100, 0.5099, 0.5098, 0.5098, 0.5097,\n",
      "        0.5096, 0.5085, 0.5085, 0.5084, 0.5084, 0.5084, 0.5084, 0.5083, 0.5082,\n",
      "        0.5082, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5078, 0.5078, 0.5078,\n",
      "        0.5078, 0.5078, 0.5078, 0.5078, 0.5078, 0.5077, 0.5077, 0.5076, 0.5074,\n",
      "        0.5074, 0.5073, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5070,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5068, 0.5068, 0.5067, 0.5067,\n",
      "        0.5067, 0.5067, 0.5067, 0.5066, 0.5066, 0.5065, 0.5063, 0.5059, 0.5057,\n",
      "        0.5056, 0.5056, 0.5054, 0.5054, 0.5053, 0.5052, 0.5052, 0.5051, 0.5049,\n",
      "        0.5049, 0.5049, 0.5048, 0.5048, 0.5048, 0.5048, 0.5048, 0.5048, 0.5048,\n",
      "        0.5047], device='cuda:0')\n",
      "tensor([0.5144, 0.5139, 0.5120, 0.5114, 0.5110, 0.5109, 0.5107, 0.5107, 0.5106,\n",
      "        0.5106, 0.5105, 0.5104, 0.5104, 0.5103, 0.5103, 0.5102, 0.5101, 0.5101,\n",
      "        0.5100, 0.5099, 0.5098, 0.5098, 0.5096, 0.5096, 0.5096, 0.5095, 0.5094,\n",
      "        0.5094, 0.5094, 0.5093, 0.5093, 0.5092, 0.5092, 0.5091, 0.5090, 0.5090,\n",
      "        0.5090, 0.5088, 0.5088, 0.5086, 0.5086, 0.5086, 0.5086, 0.5085, 0.5085,\n",
      "        0.5085, 0.5084, 0.5084, 0.5083, 0.5083, 0.5082, 0.5082, 0.5082, 0.5082,\n",
      "        0.5082, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5078, 0.5077, 0.5076, 0.5076, 0.5075, 0.5075, 0.5072, 0.5071, 0.5071,\n",
      "        0.5071, 0.5070, 0.5070, 0.5070, 0.5069, 0.5069, 0.5067, 0.5066, 0.5066,\n",
      "        0.5064, 0.5063, 0.5063, 0.5062, 0.5060, 0.5058, 0.5058, 0.5054, 0.5053,\n",
      "        0.5050], device='cuda:0')\n",
      "tensor([0.5124, 0.5114, 0.5109, 0.5109, 0.5101, 0.5099, 0.5099, 0.5099, 0.5098,\n",
      "        0.5098, 0.5086, 0.5082, 0.5082, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5076,\n",
      "        0.5074, 0.5074, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5068, 0.5067, 0.5066, 0.5066, 0.5065, 0.5064, 0.5060, 0.5057, 0.5057,\n",
      "        0.5056, 0.5054, 0.5054, 0.5053, 0.5053, 0.5052, 0.5051, 0.5049, 0.5049,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5123, 0.5113, 0.5108, 0.5108, 0.5100, 0.5099, 0.5098, 0.5098, 0.5097,\n",
      "        0.5096, 0.5084, 0.5083, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5078, 0.5074,\n",
      "        0.5074, 0.5073, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5071, 0.5070,\n",
      "        0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5067, 0.5067, 0.5066, 0.5066, 0.5065, 0.5063, 0.5059, 0.5057,\n",
      "        0.5056, 0.5056, 0.5054, 0.5054, 0.5053, 0.5052, 0.5052, 0.5051, 0.5049,\n",
      "        0.5049], device='cuda:0')\n",
      "tensor([0.5164, 0.5149, 0.5142, 0.5124, 0.5120, 0.5119, 0.5113, 0.5110, 0.5110,\n",
      "        0.5109, 0.5107, 0.5103, 0.5103, 0.5103, 0.5103, 0.5103, 0.5100, 0.5099,\n",
      "        0.5099, 0.5099, 0.5099, 0.5099, 0.5097, 0.5097, 0.5096, 0.5095, 0.5095,\n",
      "        0.5095, 0.5095, 0.5093, 0.5093, 0.5093, 0.5092, 0.5092, 0.5092, 0.5091,\n",
      "        0.5090, 0.5090, 0.5090, 0.5089, 0.5088, 0.5087, 0.5087, 0.5086, 0.5086,\n",
      "        0.5085, 0.5085, 0.5085, 0.5084, 0.5084, 0.5084, 0.5083, 0.5083, 0.5082,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5080, 0.5079, 0.5079, 0.5079, 0.5078,\n",
      "        0.5077, 0.5077, 0.5077, 0.5076, 0.5074, 0.5073, 0.5073, 0.5072, 0.5072,\n",
      "        0.5071, 0.5070, 0.5070, 0.5069, 0.5068, 0.5068, 0.5068, 0.5067, 0.5067,\n",
      "        0.5066, 0.5062, 0.5062, 0.5057, 0.5057, 0.5057, 0.5056, 0.5056, 0.5056,\n",
      "        0.5055, 0.5055, 0.5054, 0.5054, 0.5054, 0.5053, 0.5053, 0.5053, 0.5052,\n",
      "        0.5049], device='cuda:0')\n",
      "tensor([0.5124, 0.5114, 0.5109, 0.5109, 0.5101, 0.5099, 0.5099, 0.5099, 0.5098,\n",
      "        0.5098, 0.5086, 0.5082, 0.5082, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5076,\n",
      "        0.5074, 0.5074, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5068, 0.5067, 0.5066, 0.5066, 0.5065, 0.5064, 0.5060, 0.5057, 0.5057,\n",
      "        0.5056, 0.5054, 0.5054, 0.5053, 0.5053, 0.5052, 0.5051, 0.5049, 0.5049,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5124, 0.5114, 0.5109, 0.5109, 0.5101, 0.5099, 0.5099, 0.5099, 0.5098,\n",
      "        0.5098, 0.5086, 0.5082, 0.5082, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5076,\n",
      "        0.5074, 0.5074, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5068, 0.5067, 0.5066, 0.5066, 0.5065, 0.5064, 0.5060, 0.5057, 0.5057,\n",
      "        0.5056, 0.5054, 0.5054, 0.5053, 0.5053, 0.5052, 0.5051, 0.5049, 0.5049,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5123, 0.5113, 0.5108, 0.5108, 0.5100, 0.5099, 0.5098, 0.5098, 0.5097,\n",
      "        0.5096, 0.5084, 0.5083, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082,\n",
      "        0.5082, 0.5082, 0.5082, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5078, 0.5074,\n",
      "        0.5074, 0.5073, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5071, 0.5071,\n",
      "        0.5071, 0.5071, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070,\n",
      "        0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5067, 0.5067, 0.5066, 0.5066, 0.5065, 0.5063, 0.5059, 0.5057,\n",
      "        0.5056, 0.5056, 0.5054, 0.5054, 0.5053, 0.5052, 0.5052, 0.5051, 0.5049,\n",
      "        0.5049], device='cuda:0')\n",
      "tensor([0.5124, 0.5114, 0.5109, 0.5109, 0.5101, 0.5099, 0.5099, 0.5099, 0.5098,\n",
      "        0.5098, 0.5086, 0.5082, 0.5082, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5076,\n",
      "        0.5074, 0.5074, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5068, 0.5067, 0.5066, 0.5066, 0.5065, 0.5064, 0.5060, 0.5057, 0.5057,\n",
      "        0.5056, 0.5054, 0.5054, 0.5053, 0.5053, 0.5052, 0.5051, 0.5049, 0.5049,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5131, 0.5118, 0.5117, 0.5115, 0.5111, 0.5107, 0.5106, 0.5106, 0.5106,\n",
      "        0.5104, 0.5103, 0.5103, 0.5103, 0.5102, 0.5102, 0.5100, 0.5100, 0.5099,\n",
      "        0.5098, 0.5097, 0.5096, 0.5096, 0.5095, 0.5091, 0.5090, 0.5090, 0.5089,\n",
      "        0.5089, 0.5088, 0.5087, 0.5087, 0.5086, 0.5085, 0.5083, 0.5083, 0.5083,\n",
      "        0.5083, 0.5083, 0.5082, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5077,\n",
      "        0.5076, 0.5076, 0.5073, 0.5073, 0.5072, 0.5071, 0.5071, 0.5071, 0.5071,\n",
      "        0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5067, 0.5067, 0.5067, 0.5066,\n",
      "        0.5066, 0.5064, 0.5063, 0.5062, 0.5061, 0.5058, 0.5058, 0.5056, 0.5056,\n",
      "        0.5055], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5124, 0.5114, 0.5109, 0.5109, 0.5101, 0.5099, 0.5099, 0.5099, 0.5098,\n",
      "        0.5098, 0.5086, 0.5082, 0.5082, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5076,\n",
      "        0.5074, 0.5074, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5068, 0.5067, 0.5066, 0.5066, 0.5065, 0.5064, 0.5060, 0.5057, 0.5057,\n",
      "        0.5056, 0.5054, 0.5054, 0.5053, 0.5053, 0.5052, 0.5051, 0.5049, 0.5049,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5123, 0.5113, 0.5108, 0.5108, 0.5100, 0.5099, 0.5098, 0.5098, 0.5097,\n",
      "        0.5096, 0.5084, 0.5083, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079, 0.5078, 0.5074,\n",
      "        0.5074, 0.5073, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5071, 0.5070,\n",
      "        0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070,\n",
      "        0.5070, 0.5070, 0.5070, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5067, 0.5067, 0.5066, 0.5066, 0.5065, 0.5063, 0.5059, 0.5057,\n",
      "        0.5056, 0.5056, 0.5054, 0.5054, 0.5053, 0.5052, 0.5052, 0.5051, 0.5049,\n",
      "        0.5049], device='cuda:0')\n",
      "tensor([0.5123, 0.5113, 0.5108, 0.5108, 0.5100, 0.5099, 0.5098, 0.5098, 0.5097,\n",
      "        0.5096, 0.5084, 0.5083, 0.5083, 0.5083, 0.5083, 0.5082, 0.5082, 0.5082,\n",
      "        0.5082, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5078, 0.5074,\n",
      "        0.5074, 0.5073, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5070,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5068, 0.5068,\n",
      "        0.5068, 0.5068, 0.5067, 0.5067, 0.5067, 0.5067, 0.5067, 0.5066, 0.5066,\n",
      "        0.5065, 0.5063, 0.5059, 0.5057, 0.5056, 0.5056, 0.5054, 0.5054, 0.5053,\n",
      "        0.5052, 0.5052, 0.5051, 0.5049, 0.5049, 0.5048, 0.5048, 0.5048, 0.5048,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5123, 0.5114, 0.5109, 0.5107, 0.5106, 0.5106, 0.5105, 0.5105, 0.5103,\n",
      "        0.5102, 0.5102, 0.5100, 0.5100, 0.5099, 0.5098, 0.5098, 0.5096, 0.5095,\n",
      "        0.5094, 0.5093, 0.5093, 0.5093, 0.5093, 0.5092, 0.5091, 0.5091, 0.5090,\n",
      "        0.5090, 0.5090, 0.5090, 0.5089, 0.5088, 0.5088, 0.5087, 0.5087, 0.5087,\n",
      "        0.5086, 0.5086, 0.5085, 0.5085, 0.5085, 0.5085, 0.5084, 0.5084, 0.5084,\n",
      "        0.5084, 0.5083, 0.5083, 0.5083, 0.5082, 0.5082, 0.5082, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5079, 0.5079, 0.5079, 0.5079, 0.5078, 0.5076, 0.5074,\n",
      "        0.5074, 0.5073, 0.5073, 0.5072, 0.5070, 0.5070, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5067, 0.5067, 0.5067, 0.5066, 0.5066, 0.5066, 0.5065,\n",
      "        0.5064, 0.5063, 0.5063, 0.5063, 0.5063, 0.5062, 0.5062, 0.5061, 0.5061,\n",
      "        0.5060], device='cuda:0')\n",
      "tensor([0.5123, 0.5113, 0.5108, 0.5108, 0.5100, 0.5099, 0.5098, 0.5098, 0.5097,\n",
      "        0.5096, 0.5084, 0.5083, 0.5082, 0.5082, 0.5082, 0.5082, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079, 0.5079, 0.5078, 0.5074,\n",
      "        0.5074, 0.5073, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5070,\n",
      "        0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5067, 0.5067, 0.5066, 0.5066, 0.5065, 0.5063, 0.5059, 0.5057,\n",
      "        0.5056, 0.5056, 0.5054, 0.5054, 0.5053, 0.5052, 0.5052, 0.5051, 0.5049,\n",
      "        0.5049], device='cuda:0')\n",
      "tensor([0.5124, 0.5114, 0.5109, 0.5109, 0.5101, 0.5099, 0.5099, 0.5099, 0.5098,\n",
      "        0.5098, 0.5086, 0.5082, 0.5082, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5076,\n",
      "        0.5074, 0.5074, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5068, 0.5067, 0.5066, 0.5066, 0.5065, 0.5064, 0.5060, 0.5057, 0.5057,\n",
      "        0.5056, 0.5054, 0.5054, 0.5053, 0.5053, 0.5052, 0.5051, 0.5049, 0.5049,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5117, 0.5117, 0.5112, 0.5107, 0.5106, 0.5105, 0.5103, 0.5103, 0.5103,\n",
      "        0.5102, 0.5102, 0.5102, 0.5100, 0.5100, 0.5100, 0.5098, 0.5098, 0.5096,\n",
      "        0.5096, 0.5095, 0.5095, 0.5095, 0.5093, 0.5093, 0.5093, 0.5093, 0.5090,\n",
      "        0.5089, 0.5089, 0.5089, 0.5088, 0.5088, 0.5088, 0.5087, 0.5086, 0.5086,\n",
      "        0.5085, 0.5085, 0.5083, 0.5083, 0.5083, 0.5083, 0.5083, 0.5082, 0.5082,\n",
      "        0.5082, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5078, 0.5078, 0.5078, 0.5077, 0.5077, 0.5077, 0.5076, 0.5075, 0.5075,\n",
      "        0.5075, 0.5075, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5068, 0.5067, 0.5065, 0.5064, 0.5064, 0.5063, 0.5063,\n",
      "        0.5063, 0.5060, 0.5059, 0.5059, 0.5058, 0.5058, 0.5057, 0.5057, 0.5056,\n",
      "        0.5056], device='cuda:0')\n",
      "tensor([0.5123, 0.5113, 0.5108, 0.5108, 0.5100, 0.5099, 0.5098, 0.5098, 0.5097,\n",
      "        0.5096, 0.5084, 0.5083, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079, 0.5078, 0.5074,\n",
      "        0.5074, 0.5073, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5071, 0.5071,\n",
      "        0.5071, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070, 0.5070,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5067, 0.5067, 0.5066, 0.5066, 0.5065, 0.5063, 0.5059, 0.5057,\n",
      "        0.5056, 0.5056, 0.5054, 0.5054, 0.5053, 0.5052, 0.5052, 0.5051, 0.5049,\n",
      "        0.5049], device='cuda:0')\n",
      "tensor([0.5140, 0.5131, 0.5131, 0.5128, 0.5119, 0.5114, 0.5114, 0.5114, 0.5113,\n",
      "        0.5113, 0.5111, 0.5110, 0.5110, 0.5109, 0.5107, 0.5107, 0.5106, 0.5106,\n",
      "        0.5106, 0.5106, 0.5106, 0.5105, 0.5105, 0.5104, 0.5104, 0.5104, 0.5103,\n",
      "        0.5102, 0.5101, 0.5101, 0.5101, 0.5099, 0.5098, 0.5098, 0.5097, 0.5096,\n",
      "        0.5095, 0.5095, 0.5094, 0.5094, 0.5093, 0.5093, 0.5091, 0.5091, 0.5091,\n",
      "        0.5091, 0.5091, 0.5091, 0.5090, 0.5087, 0.5086, 0.5084, 0.5084, 0.5084,\n",
      "        0.5084, 0.5083, 0.5082, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5077, 0.5075, 0.5075, 0.5075,\n",
      "        0.5074, 0.5073, 0.5072, 0.5072, 0.5072, 0.5072, 0.5071, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5068, 0.5066,\n",
      "        0.5065], device='cuda:0')\n",
      "tensor([0.5124, 0.5114, 0.5109, 0.5109, 0.5101, 0.5099, 0.5099, 0.5099, 0.5098,\n",
      "        0.5098, 0.5086, 0.5082, 0.5082, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5076,\n",
      "        0.5074, 0.5074, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5068, 0.5067, 0.5066, 0.5066, 0.5065, 0.5064, 0.5060, 0.5057, 0.5057,\n",
      "        0.5056, 0.5054, 0.5054, 0.5053, 0.5053, 0.5052, 0.5051, 0.5049, 0.5049,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5123, 0.5113, 0.5108, 0.5108, 0.5100, 0.5099, 0.5098, 0.5098, 0.5097,\n",
      "        0.5096, 0.5085, 0.5084, 0.5084, 0.5083, 0.5083, 0.5083, 0.5083, 0.5083,\n",
      "        0.5083, 0.5083, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081,\n",
      "        0.5081, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5078, 0.5074,\n",
      "        0.5074, 0.5073, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5071, 0.5071,\n",
      "        0.5071, 0.5071, 0.5070, 0.5070, 0.5070, 0.5070, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5068, 0.5068, 0.5068,\n",
      "        0.5067, 0.5067, 0.5066, 0.5066, 0.5065, 0.5063, 0.5059, 0.5057, 0.5056,\n",
      "        0.5056, 0.5054, 0.5054, 0.5053, 0.5052, 0.5052, 0.5051, 0.5048, 0.5048,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5123, 0.5113, 0.5108, 0.5108, 0.5100, 0.5099, 0.5098, 0.5098, 0.5097,\n",
      "        0.5096, 0.5084, 0.5084, 0.5084, 0.5083, 0.5083, 0.5083, 0.5083, 0.5083,\n",
      "        0.5083, 0.5083, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5078, 0.5074,\n",
      "        0.5074, 0.5073, 0.5072, 0.5072, 0.5072, 0.5072, 0.5072, 0.5072, 0.5072,\n",
      "        0.5072, 0.5071, 0.5071, 0.5071, 0.5071, 0.5070, 0.5070, 0.5070, 0.5070,\n",
      "        0.5070, 0.5070, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5068, 0.5067, 0.5067, 0.5066, 0.5066, 0.5065, 0.5063, 0.5059, 0.5057,\n",
      "        0.5056, 0.5056, 0.5054, 0.5054, 0.5053, 0.5052, 0.5052, 0.5051, 0.5049,\n",
      "        0.5049], device='cuda:0')\n",
      "tensor([0.5124, 0.5114, 0.5109, 0.5109, 0.5101, 0.5099, 0.5099, 0.5099, 0.5098,\n",
      "        0.5098, 0.5086, 0.5082, 0.5082, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5076,\n",
      "        0.5074, 0.5074, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5068, 0.5067, 0.5066, 0.5066, 0.5065, 0.5064, 0.5060, 0.5057, 0.5057,\n",
      "        0.5056, 0.5054, 0.5054, 0.5053, 0.5053, 0.5052, 0.5051, 0.5049, 0.5049,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5124, 0.5114, 0.5109, 0.5109, 0.5101, 0.5099, 0.5099, 0.5099, 0.5098,\n",
      "        0.5098, 0.5086, 0.5082, 0.5082, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5076,\n",
      "        0.5074, 0.5074, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5068, 0.5067, 0.5066, 0.5066, 0.5065, 0.5064, 0.5060, 0.5057, 0.5057,\n",
      "        0.5056, 0.5054, 0.5054, 0.5053, 0.5053, 0.5052, 0.5051, 0.5049, 0.5049,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5134, 0.5126, 0.5123, 0.5116, 0.5113, 0.5112, 0.5112, 0.5109, 0.5108,\n",
      "        0.5105, 0.5099, 0.5098, 0.5098, 0.5098, 0.5096, 0.5095, 0.5094, 0.5094,\n",
      "        0.5093, 0.5091, 0.5090, 0.5088, 0.5088, 0.5087, 0.5086, 0.5086, 0.5085,\n",
      "        0.5084, 0.5084, 0.5084, 0.5083, 0.5083, 0.5082, 0.5082, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5075, 0.5075, 0.5074, 0.5074, 0.5074, 0.5073, 0.5073,\n",
      "        0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5070, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5065,\n",
      "        0.5065, 0.5064, 0.5063, 0.5059, 0.5059, 0.5057, 0.5055, 0.5054, 0.5051,\n",
      "        0.5050], device='cuda:0')\n",
      "tensor([0.5123, 0.5113, 0.5108, 0.5108, 0.5100, 0.5099, 0.5098, 0.5098, 0.5097,\n",
      "        0.5096, 0.5084, 0.5084, 0.5084, 0.5083, 0.5083, 0.5082, 0.5082, 0.5082,\n",
      "        0.5082, 0.5082, 0.5082, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5078, 0.5074, 0.5074, 0.5073,\n",
      "        0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5070, 0.5070, 0.5070,\n",
      "        0.5070, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5067, 0.5067,\n",
      "        0.5067, 0.5066, 0.5066, 0.5065, 0.5063, 0.5059, 0.5057, 0.5056, 0.5056,\n",
      "        0.5054, 0.5054, 0.5053, 0.5052, 0.5052, 0.5051, 0.5049, 0.5049, 0.5048,\n",
      "        0.5048, 0.5047, 0.5047, 0.5047, 0.5047, 0.5046, 0.5046, 0.5046, 0.5046,\n",
      "        0.5046], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5139, 0.5110, 0.5106, 0.5106, 0.5106, 0.5105, 0.5104, 0.5104, 0.5103,\n",
      "        0.5102, 0.5102, 0.5099, 0.5099, 0.5098, 0.5098, 0.5097, 0.5097, 0.5097,\n",
      "        0.5096, 0.5096, 0.5095, 0.5094, 0.5094, 0.5093, 0.5093, 0.5093, 0.5093,\n",
      "        0.5093, 0.5090, 0.5089, 0.5089, 0.5087, 0.5087, 0.5087, 0.5086, 0.5086,\n",
      "        0.5086, 0.5085, 0.5084, 0.5084, 0.5083, 0.5082, 0.5082, 0.5082, 0.5082,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5078, 0.5077, 0.5077, 0.5077, 0.5076, 0.5074, 0.5073, 0.5071, 0.5070,\n",
      "        0.5070, 0.5069, 0.5069, 0.5069, 0.5069, 0.5068, 0.5067, 0.5067, 0.5067,\n",
      "        0.5067, 0.5067, 0.5064, 0.5062, 0.5062, 0.5062, 0.5057, 0.5055, 0.5055,\n",
      "        0.5055, 0.5054, 0.5054, 0.5053, 0.5052, 0.5051, 0.5051, 0.5051, 0.5051,\n",
      "        0.5051], device='cuda:0')\n",
      "tensor([0.5151, 0.5142, 0.5141, 0.5125, 0.5124, 0.5119, 0.5119, 0.5118, 0.5117,\n",
      "        0.5116, 0.5114, 0.5110, 0.5109, 0.5106, 0.5104, 0.5104, 0.5101, 0.5099,\n",
      "        0.5098, 0.5091, 0.5091, 0.5091, 0.5090, 0.5089, 0.5088, 0.5086, 0.5086,\n",
      "        0.5086, 0.5085, 0.5085, 0.5085, 0.5084, 0.5084, 0.5084, 0.5084, 0.5083,\n",
      "        0.5083, 0.5083, 0.5082, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5078, 0.5078, 0.5078,\n",
      "        0.5078, 0.5078, 0.5078, 0.5078, 0.5077, 0.5077, 0.5077, 0.5077, 0.5077,\n",
      "        0.5077, 0.5076, 0.5075, 0.5075, 0.5074, 0.5074, 0.5073, 0.5072, 0.5072,\n",
      "        0.5072, 0.5071, 0.5070, 0.5070, 0.5070, 0.5070, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5068, 0.5068, 0.5067, 0.5067, 0.5066, 0.5064, 0.5061,\n",
      "        0.5061], device='cuda:0')\n",
      "tensor([0.5124, 0.5114, 0.5109, 0.5109, 0.5101, 0.5099, 0.5099, 0.5099, 0.5098,\n",
      "        0.5098, 0.5086, 0.5082, 0.5082, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5076,\n",
      "        0.5074, 0.5074, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5068, 0.5067, 0.5066, 0.5066, 0.5065, 0.5064, 0.5060, 0.5057, 0.5057,\n",
      "        0.5056, 0.5054, 0.5054, 0.5053, 0.5053, 0.5052, 0.5051, 0.5049, 0.5049,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5115, 0.5111, 0.5111, 0.5106, 0.5105, 0.5105, 0.5104, 0.5104, 0.5104,\n",
      "        0.5104, 0.5103, 0.5103, 0.5103, 0.5102, 0.5102, 0.5102, 0.5101, 0.5101,\n",
      "        0.5100, 0.5099, 0.5098, 0.5096, 0.5095, 0.5095, 0.5093, 0.5092, 0.5091,\n",
      "        0.5091, 0.5090, 0.5090, 0.5089, 0.5088, 0.5088, 0.5088, 0.5088, 0.5087,\n",
      "        0.5086, 0.5086, 0.5085, 0.5084, 0.5083, 0.5083, 0.5083, 0.5083, 0.5083,\n",
      "        0.5083, 0.5082, 0.5082, 0.5082, 0.5082, 0.5081, 0.5081, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5077, 0.5076, 0.5074, 0.5073, 0.5073, 0.5070, 0.5068, 0.5068,\n",
      "        0.5068, 0.5068, 0.5068, 0.5068, 0.5067, 0.5067, 0.5064, 0.5064, 0.5063,\n",
      "        0.5063, 0.5061, 0.5060, 0.5060, 0.5059, 0.5059, 0.5058, 0.5058, 0.5058,\n",
      "        0.5057, 0.5056, 0.5056, 0.5055, 0.5055, 0.5054, 0.5054, 0.5054, 0.5054,\n",
      "        0.5054], device='cuda:0')\n",
      "tensor([0.5144, 0.5141, 0.5122, 0.5115, 0.5110, 0.5110, 0.5109, 0.5109, 0.5107,\n",
      "        0.5107, 0.5106, 0.5105, 0.5105, 0.5104, 0.5104, 0.5103, 0.5103, 0.5101,\n",
      "        0.5101, 0.5101, 0.5099, 0.5099, 0.5096, 0.5096, 0.5096, 0.5096, 0.5096,\n",
      "        0.5095, 0.5093, 0.5093, 0.5092, 0.5092, 0.5092, 0.5091, 0.5091, 0.5091,\n",
      "        0.5089, 0.5088, 0.5088, 0.5088, 0.5087, 0.5087, 0.5086, 0.5086, 0.5086,\n",
      "        0.5085, 0.5085, 0.5084, 0.5083, 0.5083, 0.5082, 0.5082, 0.5082, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079, 0.5078, 0.5078, 0.5076,\n",
      "        0.5076, 0.5076, 0.5075, 0.5073, 0.5072, 0.5071, 0.5071, 0.5070, 0.5070,\n",
      "        0.5069, 0.5068, 0.5067, 0.5066, 0.5064, 0.5064, 0.5063, 0.5062, 0.5061,\n",
      "        0.5061, 0.5059, 0.5059, 0.5054, 0.5053, 0.5051, 0.5050, 0.5050, 0.5050,\n",
      "        0.5049], device='cuda:0')\n",
      "tensor([0.5124, 0.5114, 0.5109, 0.5109, 0.5101, 0.5099, 0.5099, 0.5099, 0.5098,\n",
      "        0.5098, 0.5086, 0.5082, 0.5082, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5076,\n",
      "        0.5074, 0.5074, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5068, 0.5067, 0.5066, 0.5066, 0.5065, 0.5064, 0.5060, 0.5057, 0.5057,\n",
      "        0.5056, 0.5054, 0.5054, 0.5053, 0.5053, 0.5052, 0.5051, 0.5049, 0.5049,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5124, 0.5114, 0.5109, 0.5109, 0.5101, 0.5099, 0.5099, 0.5099, 0.5098,\n",
      "        0.5098, 0.5086, 0.5082, 0.5082, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079,\n",
      "        0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5079, 0.5076,\n",
      "        0.5074, 0.5074, 0.5072, 0.5072, 0.5072, 0.5071, 0.5071, 0.5070, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5068, 0.5067, 0.5066, 0.5066, 0.5065, 0.5064, 0.5060, 0.5057, 0.5057,\n",
      "        0.5056, 0.5054, 0.5054, 0.5053, 0.5053, 0.5052, 0.5051, 0.5049, 0.5049,\n",
      "        0.5048], device='cuda:0')\n",
      "tensor([0.5123, 0.5113, 0.5108, 0.5108, 0.5100, 0.5099, 0.5098, 0.5098, 0.5097,\n",
      "        0.5096, 0.5084, 0.5083, 0.5083, 0.5083, 0.5083, 0.5083, 0.5083, 0.5083,\n",
      "        0.5083, 0.5083, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082, 0.5082, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081, 0.5081,\n",
      "        0.5081, 0.5081, 0.5081, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080,\n",
      "        0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5080, 0.5079, 0.5078, 0.5074,\n",
      "        0.5074, 0.5073, 0.5073, 0.5072, 0.5072, 0.5072, 0.5072, 0.5072, 0.5072,\n",
      "        0.5072, 0.5071, 0.5071, 0.5071, 0.5071, 0.5071, 0.5070, 0.5070, 0.5070,\n",
      "        0.5070, 0.5070, 0.5070, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069, 0.5069,\n",
      "        0.5069, 0.5067, 0.5067, 0.5066, 0.5066, 0.5065, 0.5063, 0.5059, 0.5057,\n",
      "        0.5056, 0.5056, 0.5054, 0.5054, 0.5053, 0.5052, 0.5052, 0.5051, 0.5049,\n",
      "        0.5049], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-82e8e066f37c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ba7edfc697e0>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mscan_pixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pixels_hu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mpix_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscan_pixels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0msegmented_lungs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_lung_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpix_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mnormal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmented_lungs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-91d3462d70d9>\u001b[0m in \u001b[0;36msegment_lung_mask\u001b[0;34m(image, fill_lung_structures)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# 0 is treated as background, which we do not want\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mbinary_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Pick the pixel in the very corner to determine which label is air.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/lib/python3.7/site-packages/skimage/measure/_label.py\u001b[0m in \u001b[0;36mlabel\u001b[0;34m(input, neighbors, background, return_num, connectivity)\u001b[0m\n\u001b[1;32m     91\u001b[0m      [0 0 0]]\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mclabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnectivity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "total = 0\n",
    "for images, targets in (dataloader['test']):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        images = move_to(images,device)\n",
    "        pred = model(images) # Pass the image to the model\n",
    "    for i, img in enumerate(images):\n",
    "        total += 1\n",
    "        list_box = []\n",
    "        for lab in targets[i]['boxes'].numpy():\n",
    "            list_box.append(lab[0]) #xmin\n",
    "            list_box.append(lab[1]) #ymin\n",
    "            list_box.append(lab[2]) #xmax\n",
    "            list_box.append(lab[3]) #ymax\n",
    "            \n",
    "        out = pred[i]\n",
    "        pred_boxes = [[i[0], i[1], i[2], i[3]] for i in list(out['boxes'].cpu().detach().numpy())]\n",
    "        print(out['scores'])\n",
    "        pred_score = list(out['scores'].cpu().detach().numpy())\n",
    "        pred_class = [classes[i] for i in list(out['labels'].cpu().numpy())]\n",
    "\n",
    "#         max_score_index = pred_score.index(max(pred_score))\n",
    "#         max_score = pred_boxes[max_score_index]\n",
    "        \n",
    "#         if (bb_intersection_over_union((list_box),max_score) > 0):\n",
    "#             count += 1\n",
    "#             print(\"iou: \", bb_intersection_over_union((list_box),max_score))\n",
    "#             print(\"prediction box area: \", (max_score[2] - max_score[0]) * (max_score[3] - max_score[1]), \"box area: \", (list_box[2] - list_box[0]) * (list_box[3] - list_box[1]))\n",
    "#             fig,ax = plt.subplots(1, figsize=(5,5))\n",
    "#             ax.imshow(img[0].cpu().squeeze().numpy(), cmap=\"gray\")\n",
    "#             label = patches.Rectangle((list_box[0], list_box[1] ),(list_box[2] - list_box[0]),(list_box[3] - list_box[1]),linewidth=1,edgecolor='r',facecolor='none')\n",
    "#             maxScore = patches.Rectangle((max_score[0], max_score[1] ),(max_score[2] - max_score[0]),(max_score[3] - max_score[1]),linewidth=1,edgecolor='y',facecolor='none')\n",
    "#             ax.add_patch(label)\n",
    "#             ax.add_patch(maxScore)\n",
    "#             plt.show()\n",
    "            \n",
    "print(count, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
